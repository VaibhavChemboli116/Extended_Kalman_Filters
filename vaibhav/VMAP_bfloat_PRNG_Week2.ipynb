{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VMAP"
      ],
      "metadata": {
        "id": "i8IZ3TrdfquV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, vmap\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Setup: 32 attention heads, each with 64x64 query-key matrices\n",
        "batch_size = 32\n",
        "matrix_size = 64\n",
        "\n",
        "# Create random matrices representing attention queries and keys\n",
        "key = random.PRNGKey(0)\n",
        "queries = random.normal(key, (batch_size, matrix_size, matrix_size))\n",
        "keys = random.normal(key, (batch_size, matrix_size, matrix_size))\n",
        "\n",
        "# Method 1: Manual loop (slow, not idiomatic JAX)\n",
        "def matmul_loop(queries, keys):\n",
        "    results = []\n",
        "    for i in range(queries.shape[0]):\n",
        "        results.append(jnp.matmul(queries[i], keys[i]))\n",
        "    return jnp.stack(results)\n",
        "\n",
        "# Method 2: Using vmap (fast, vectorized, parallelizable)\n",
        "def matmul_single(q, k):\n",
        "    return jnp.matmul(q, k)\n",
        "\n",
        "# vmap automatically vectorizes over the first dimension\n",
        "matmul_batch = vmap(matmul_single)\n",
        "\n",
        "# Benchmark\n",
        "_ = matmul_loop(queries[:2], keys[:2])  # Small warmup\n",
        "_ = matmul_batch(queries[:2], keys[:2])  # Small warmup\n",
        "jax.block_until_ready(_)  # Wait for GPU\n",
        "\n",
        "print(\"\\nTiming 100 iterations...\")\n",
        "\n",
        "# Time loop version\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    result_loop = matmul_loop(queries, keys)\n",
        "    jax.block_until_ready(result_loop)\n",
        "loop_time = time.time() - start\n",
        "\n",
        "# Time vmap version\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    result_vmap = matmul_batch(queries, keys)\n",
        "    jax.block_until_ready(result_vmap)\n",
        "vmap_time = time.time() - start\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"   Loop approach: {loop_time:.4f}s\")\n",
        "print(f\"   vmap approach: {vmap_time:.4f}s\")\n",
        "print(f\"   Speedup: {loop_time/vmap_time:.2f}x faster!\")\n",
        "print(f\"\\nResults match: {jnp.allclose(result_loop, result_vmap)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S42jFE4Fip8L",
        "outputId": "8e4fbded-2147-4e1b-9b92-910a9acbfb63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Timing 100 iterations...\n",
            "\n",
            "Results:\n",
            "   Loop approach: 2.1449s\n",
            "   vmap approach: 0.1052s\n",
            "   Speedup: 20.39x faster!\n",
            "\n",
            "Results match: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRNG"
      ],
      "metadata": {
        "id": "h1socl84fvbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# BAD: Reusing the same key (produces identical masks!)\n",
        "def dropout_bad(x, key, drop_rate=0.5):\n",
        "    masks = []\n",
        "    for i in range(5):  # 5 layers\n",
        "        #Using same key repeatedly - ALL MASKS ARE IDENTICAL!\n",
        "        mask = random.bernoulli(key, 1 - drop_rate, x.shape)\n",
        "        masks.append(mask)\n",
        "    return masks\n",
        "\n",
        "# GOOD: Proper key splitting (produces independent masks)\n",
        "def dropout_good(x, key, drop_rate=0.5):\n",
        "    \"\"\"CORRECT: Splitting key gives independent randomness\"\"\"\n",
        "    masks = []\n",
        "    for i in range(5):  # 5 layers\n",
        "        # Split key to get independent randomness for each layer\n",
        "        key, subkey = random.split(key)\n",
        "        mask = random.bernoulli(subkey, 1 - drop_rate, x.shape)\n",
        "        masks.append(mask)\n",
        "    return masks\n",
        "\n",
        "# BEST: Vectorized key splitting with vmap\n",
        "def dropout_best(x, key, drop_rate=0.5, num_layers=5):\n",
        "    \"\"\"OPTIMAL: Vectorized key splitting + parallel generation\"\"\"\n",
        "    # Split key into num_layers independent subkeys at once\n",
        "    keys = random.split(key, num_layers)\n",
        "\n",
        "    # Vectorize mask generation across all keys\n",
        "    def make_mask(subkey):\n",
        "        return random.bernoulli(subkey, 1 - drop_rate, x.shape)\n",
        "\n",
        "    # vmap over keys dimension to generate all masks in parallel\n",
        "    masks = vmap(make_mask)(keys)\n",
        "    return masks\n",
        "\n",
        "# Demo\n",
        "print(\"Generating dropout masks for 5 layers...\\n\")\n",
        "key = random.PRNGKey(42)\n",
        "x = jnp.ones((4, 10))  # Batch of 4, feature size 10\n",
        "\n",
        "masks_bad = dropout_bad(x, key)\n",
        "masks_good = dropout_good(x, key)\n",
        "masks_best = dropout_best(x, key)\n",
        "\n",
        "print(\"BAD (reusing key) - First 3 masks:\")\n",
        "for i in range(3):\n",
        "    print(f\"   Layer {i}: {masks_bad[i][0, :5].astype(int)}...\")\n",
        "print(\"   PROBLEM: All masks are IDENTICAL!\")\n",
        "\n",
        "print(\"\\nGOOD (splitting key) - First 3 masks:\")\n",
        "for i in range(3):\n",
        "    print(f\"   Layer {i}: {masks_good[i][0, :5].astype(int)}...\")\n",
        "print(\"   Each layer has independent randomness\")\n",
        "\n",
        "print(\"\\nBEST (vectorized splitting) - First 3 masks:\")\n",
        "for i in range(3):\n",
        "    print(f\"   Layer {i}: {masks_best[i][0, :5].astype(int)}...\")\n",
        "print(\"   Independent + parallelized generation\")\n",
        "\n",
        "# Verify independence\n",
        "print(\"\\nStatistical verification:\")\n",
        "correlation_bad = jnp.corrcoef(masks_bad[0].flatten(), masks_bad[1].flatten())[0, 1]\n",
        "correlation_good = jnp.corrcoef(masks_good[0].flatten(), masks_good[1].flatten())[0, 1]\n",
        "\n",
        "print(f\"   Correlation between layers (bad):  {correlation_bad:.4f} (should be ~0)\")\n",
        "print(f\"   Correlation between layers (good): {correlation_good:.4f} (actually independent)\")\n",
        "\n",
        "check_same = lambda masks: all(jnp.allclose(masks[0], m) for m in masks[1:])\n",
        "print(f\"All BAD masks identical?  {check_same(masks_bad)}\")\n",
        "print(f\"All GOOD masks identical? {check_same(masks_good)}\\nAll BEST masks identical?  {check_same(masks_best)}\")\n",
        "\n",
        "\n",
        "#adds more randomness, model learns in a better way\n",
        "#Pytorch does vmap by default because it is not functional JAX is functional so we need to do manually\n",
        "# For key splitting it is the same reason , pytorch is a glaobal state managed framework so defining one global seed will produce similar results,\n",
        "# whereas the JAX is functional so we need to define explicitly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA8VVNHdTq_o",
        "outputId": "d7e57fcc-3699-44ff-d540-c2aedcbf5582"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dropout masks for 5 layers...\n",
            "\n",
            "BAD (reusing key) - First 3 masks:\n",
            "   Layer 0: [1 0 0 0 1]...\n",
            "   Layer 1: [1 0 0 0 1]...\n",
            "   Layer 2: [1 0 0 0 1]...\n",
            "   PROBLEM: All masks are IDENTICAL!\n",
            "\n",
            "GOOD (splitting key) - First 3 masks:\n",
            "   Layer 0: [0 0 1 1 1]...\n",
            "   Layer 1: [1 1 1 1 0]...\n",
            "   Layer 2: [1 1 1 1 0]...\n",
            "   Each layer has independent randomness\n",
            "\n",
            "BEST (vectorized splitting) - First 3 masks:\n",
            "   Layer 0: [0 1 0 0 0]...\n",
            "   Layer 1: [0 0 1 1 1]...\n",
            "   Layer 2: [0 0 1 1 1]...\n",
            "   Independent + parallelized generation\n",
            "\n",
            "Statistical verification:\n",
            "   Correlation between layers (bad):  1.0000 (should be ~0)\n",
            "   Correlation between layers (good): -0.1549 (actually independent)\n",
            "All BAD masks identical?  True\n",
            "All GOOD masks identical? False\n",
            "All BEST masks identical?  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bfloat()"
      ],
      "metadata": {
        "id": "KIKV4mIXhC2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With TPU"
      ],
      "metadata": {
        "id": "lNxdfEsdjYrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "print(jax.devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8b3iYUSi9Or",
        "outputId": "1da11103-e7f5-4807-9d2b-25c49173fd0f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple 2-layer neural network\n",
        "def create_params(key, input_size, hidden_size, output_size, dtype):\n",
        "    \"\"\"Initialize network parameters in specified dtype\"\"\"\n",
        "    k1, k2 = random.split(key)\n",
        "    w1 = random.normal(k1, (input_size, hidden_size), dtype=dtype) * 0.01\n",
        "    w2 = random.normal(k2, (hidden_size, output_size), dtype=dtype) * 0.01\n",
        "    return {'w1': w1, 'w2': w2}\n",
        "\n",
        "def forward(params, x):\n",
        "    \"\"\"Forward pass through 2-layer network\"\"\"\n",
        "    h = jnp.tanh(x @ params['w1'])  # Hidden layer\n",
        "    return h @ params['w2']  # Output layer\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    \"\"\"MSE loss\"\"\"\n",
        "    pred = forward(params, x)\n",
        "    return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "# Training setup\n",
        "key = random.PRNGKey(0)\n",
        "input_size, hidden_size, output_size = 1024, 2048, 512\n",
        "batch_size = 128\n",
        "\n",
        "# Generate dummy data\n",
        "x_data = random.normal(key, (batch_size, input_size))\n",
        "y_data = random.normal(key, (batch_size, output_size))\n",
        "\n",
        "# Create models in different precisions\n",
        "print(\"Creating models in different precisions...\\n\")\n",
        "\n",
        "params_fp32 = create_params(key, input_size, hidden_size, output_size, jnp.float32)\n",
        "params_bf16 = create_params(key, input_size, hidden_size, output_size, jnp.bfloat16)\n",
        "\n",
        "# Convert input data\n",
        "x_fp32 = x_data.astype(jnp.float32)\n",
        "x_bf16 = x_data.astype(jnp.bfloat16)\n",
        "y_fp32 = y_data.astype(jnp.float32)\n",
        "y_bf16 = y_data.astype(jnp.bfloat16)\n",
        "\n",
        "# Memory comparison\n",
        "def get_memory_mb(params):\n",
        "    \"\"\"Calculate memory usage in MB\"\"\"\n",
        "    total_bytes = sum(p.nbytes for p in jax.tree_util.tree_leaves(params))\n",
        "    return total_bytes / (1024 ** 2)\n",
        "\n",
        "mem_fp32 = get_memory_mb(params_fp32)\n",
        "mem_bf16 = get_memory_mb(params_bf16)\n",
        "\n",
        "print(f\"Memory Usage:\")\n",
        "print(f\"   float32 params: {mem_fp32:.2f} MB\")\n",
        "print(f\"   bfloat16 params: {mem_bf16:.2f} MB\")\n",
        "print(f\"   Memory saved: {mem_fp32 - mem_bf16:.2f} MB ({(1 - mem_bf16/mem_fp32)*100:.1f}%)\")\n",
        "\n",
        "# Computational speed comparison\n",
        "print(\"\\nSpeed comparison (100 forward passes)...\")\n",
        "\n",
        "# Warm up\n",
        "_ = forward(params_fp32, x_fp32)\n",
        "_ = forward(params_bf16, x_bf16)\n",
        "jax.block_until_ready(_)\n",
        "\n",
        "# Time float32\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    out_fp32 = forward(params_fp32, x_fp32)\n",
        "    jax.block_until_ready(out_fp32)\n",
        "time_fp32 = time.time() - start\n",
        "\n",
        "# Time bfloat16\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    out_bf16 = forward(params_bf16, x_bf16)\n",
        "    jax.block_until_ready(out_bf16)\n",
        "time_bf16 = time.time() - start\n",
        "\n",
        "print(f\"\\nComputation Time:\")\n",
        "print(f\"   float32:  {time_fp32:.4f}s\")\n",
        "print(f\"   bfloat16: {time_bf16:.4f}s\")\n",
        "if time_bf16 < time_fp32:\n",
        "    print(f\"   Speedup: {time_fp32/time_bf16:.2f}x faster with bfloat16!\")\n",
        "else:\n",
        "    print(f\"   Note: Speedup varies by hardware (TPU shows 2-4x gains)\")\n",
        "\n",
        "# Accuracy comparison\n",
        "print(\"\\nNumerical Accuracy:\")\n",
        "loss_fp32 = loss_fn(params_fp32, x_fp32, y_fp32)\n",
        "loss_bf16 = loss_fn(params_bf16, x_bf16, y_bf16)\n",
        "\n",
        "# Convert to Python floats to ensure proper formatting\n",
        "loss_fp32_val = float(loss_fp32)\n",
        "loss_bf16_val = float(loss_bf16)\n",
        "\n",
        "print(f\"   float32 loss:  {loss_fp32_val:.6f}\")\n",
        "print(f\"   bfloat16 loss: {loss_bf16_val:.6f}\")\n",
        "print(f\"   Difference:    {abs(loss_fp32_val - loss_bf16_val):.6f}\")\n",
        "\n",
        "\n",
        "\n",
        "# float32 (32 bits):\n",
        "# [S][EEEEEEEE][MMMMMMMMMMMMMMMMMMMMMMM]\n",
        "#  1    8 bits      23 bits\n",
        "#  ↑    exponent    mantissa (precision)\n",
        "# sign\n",
        "\n",
        "# bfloat16 (16 bits):\n",
        "# [S][EEEEEEEE][MMMMMMM]\n",
        "#  1    8 bits   7 bits\n",
        "#  ↑    exponent mantissa\n",
        "# sign\n",
        "\n",
        "# float16 (16 bits):\n",
        "# [S][EEEEE][MMMMMMMMMM]\n",
        "#  1  5 bits  10 bits\n",
        "#  ↑  exponent mantissa\n",
        "# sign\n",
        "\n",
        "### When large language models have deep layers which is more than 150+ complex layers the bfloat saves time and memory footprint with almost same efficiency in the precision which is not that much required for gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W9FSMXuTq9L",
        "outputId": "c1d5a488-7092-4de8-9bf8-1387280db2a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating models in different precisions...\n",
            "\n",
            "Memory Usage:\n",
            "   float32 params: 12.00 MB\n",
            "   bfloat16 params: 6.00 MB\n",
            "   Memory saved: 6.00 MB (50.0%)\n",
            "\n",
            "Speed comparison (100 forward passes)...\n",
            "\n",
            "Computation Time:\n",
            "   float32:  0.0242s\n",
            "   bfloat16: 0.0231s\n",
            "   Speedup: 1.05x faster with bfloat16!\n",
            "\n",
            "Numerical Accuracy:\n",
            "   float32 loss:  1.024064\n",
            "   bfloat16 loss: 1.023438\n",
            "   Difference:    0.000626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With GPU"
      ],
      "metadata": {
        "id": "h9_uTe1rjcbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "print(jax.devices())"
      ],
      "metadata": {
        "id": "XL6ge-AFTq6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc946fa-6b9e-4777-aa39-8ed715ad1bd2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CudaDevice(id=0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, vmap\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Simple 2-layer neural network\n",
        "def create_params(key, input_size, hidden_size, output_size, dtype):\n",
        "    \"\"\"Initialize network parameters in specified dtype\"\"\"\n",
        "    k1, k2 = random.split(key)\n",
        "    w1 = random.normal(k1, (input_size, hidden_size), dtype=dtype) * 0.01\n",
        "    w2 = random.normal(k2, (hidden_size, output_size), dtype=dtype) * 0.01\n",
        "    return {'w1': w1, 'w2': w2}\n",
        "\n",
        "def forward(params, x):\n",
        "    \"\"\"Forward pass through 2-layer network\"\"\"\n",
        "    h = jnp.tanh(x @ params['w1'])  # Hidden layer\n",
        "    return h @ params['w2']  # Output layer\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    \"\"\"MSE loss\"\"\"\n",
        "    pred = forward(params, x)\n",
        "    return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "# Training setup\n",
        "key = random.PRNGKey(0)\n",
        "input_size, hidden_size, output_size = 1024, 2048, 512\n",
        "batch_size = 128\n",
        "\n",
        "# Generate dummy data\n",
        "x_data = random.normal(key, (batch_size, input_size))\n",
        "y_data = random.normal(key, (batch_size, output_size))\n",
        "\n",
        "# Create models in different precisions\n",
        "print(\"Creating models in different precisions...\\n\")\n",
        "\n",
        "params_fp32 = create_params(key, input_size, hidden_size, output_size, jnp.float32)\n",
        "params_bf16 = create_params(key, input_size, hidden_size, output_size, jnp.bfloat16)\n",
        "\n",
        "# Convert input data\n",
        "x_fp32 = x_data.astype(jnp.float32)\n",
        "x_bf16 = x_data.astype(jnp.bfloat16)\n",
        "y_fp32 = y_data.astype(jnp.float32)\n",
        "y_bf16 = y_data.astype(jnp.bfloat16)\n",
        "\n",
        "# Memory comparison\n",
        "def get_memory_mb(params):\n",
        "    \"\"\"Calculate memory usage in MB\"\"\"\n",
        "    total_bytes = sum(p.nbytes for p in jax.tree_util.tree_leaves(params))\n",
        "    return total_bytes / (1024 ** 2)\n",
        "\n",
        "mem_fp32 = get_memory_mb(params_fp32)\n",
        "mem_bf16 = get_memory_mb(params_bf16)\n",
        "\n",
        "print(f\"Memory Usage:\")\n",
        "print(f\"   float32 params: {mem_fp32:.2f} MB\")\n",
        "print(f\"   bfloat16 params: {mem_bf16:.2f} MB\")\n",
        "print(f\"   Memory saved: {mem_fp32 - mem_bf16:.2f} MB ({(1 - mem_bf16/mem_fp32)*100:.1f}%)\")\n",
        "\n",
        "# Computational speed comparison\n",
        "print(\"\\nSpeed comparison (100 forward passes)...\")\n",
        "\n",
        "# Warm up\n",
        "_ = forward(params_fp32, x_fp32)\n",
        "_ = forward(params_bf16, x_bf16)\n",
        "jax.block_until_ready(_)\n",
        "\n",
        "# Time float32\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    out_fp32 = forward(params_fp32, x_fp32)\n",
        "    jax.block_until_ready(out_fp32)\n",
        "time_fp32 = time.time() - start\n",
        "\n",
        "# Time bfloat16\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    out_bf16 = forward(params_bf16, x_bf16)\n",
        "    jax.block_until_ready(out_bf16)\n",
        "time_bf16 = time.time() - start\n",
        "\n",
        "print(f\"\\nComputation Time:\")\n",
        "print(f\"   float32:  {time_fp32:.4f}s\")\n",
        "print(f\"   bfloat16: {time_bf16:.4f}s\")\n",
        "if time_bf16 < time_fp32:\n",
        "    print(f\"   Speedup: {time_fp32/time_bf16:.2f}x faster with bfloat16!\")\n",
        "else:\n",
        "    print(f\"   Note: Speedup varies by hardware (TPU shows 2-4x gains)\")\n",
        "\n",
        "# Accuracy comparison\n",
        "print(\"\\nNumerical Accuracy:\")\n",
        "loss_fp32 = loss_fn(params_fp32, x_fp32, y_fp32)\n",
        "loss_bf16 = loss_fn(params_bf16, x_bf16, y_bf16)\n",
        "\n",
        "# Convert to Python floats to ensure proper formatting\n",
        "loss_fp32_val = float(loss_fp32)\n",
        "loss_bf16_val = float(loss_bf16)\n",
        "\n",
        "print(f\"   float32 loss:  {loss_fp32_val:.6f}\")\n",
        "print(f\"   bfloat16 loss: {loss_bf16_val:.6f}\")\n",
        "print(f\"   Difference:    {abs(loss_fp32_val - loss_bf16_val):.6f}\")\n"
      ],
      "metadata": {
        "id": "if40Ne27Tq2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b568999e-e02d-45aa-f1a3-ec325608ce41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating models in different precisions...\n",
            "\n",
            "Memory Usage:\n",
            "   float32 params: 12.00 MB\n",
            "   bfloat16 params: 6.00 MB\n",
            "   Memory saved: 6.00 MB (50.0%)\n",
            "\n",
            "Speed comparison (100 forward passes)...\n",
            "\n",
            "Computation Time:\n",
            "   float32:  0.0564s\n",
            "   bfloat16: 0.1586s\n",
            "   Note: Speedup varies by hardware (TPU shows 2-4x gains)\n",
            "\n",
            "Numerical Accuracy:\n",
            "   float32 loss:  1.024060\n",
            "   bfloat16 loss: 1.023438\n",
            "   Difference:    0.000623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bfloat is not saving time on GPU's?"
      ],
      "metadata": {
        "id": "TQEW0MNFkwYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdC3H2idkwPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}