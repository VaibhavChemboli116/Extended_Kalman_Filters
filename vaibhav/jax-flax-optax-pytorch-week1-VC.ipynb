{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Key splitting option given for multi devices parrallelism which is not there in PyTorch\n# Still a bit unclear the exact advantage of splitting where they can define multiple key for multiple devices.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:02.661797Z","iopub.execute_input":"2025-10-07T23:51:02.662482Z","iopub.status.idle":"2025-10-07T23:51:02.666284Z","shell.execute_reply.started":"2025-10-07T23:51:02.662459Z","shell.execute_reply":"2025-10-07T23:51:02.665576Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\n# Load dataset\ndata = load_breast_cancer()\n\n# Convert to DataFrame\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf[\"target\"] = data.target  # add target column\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:02.671049Z","iopub.execute_input":"2025-10-07T23:51:02.671279Z","iopub.status.idle":"2025-10-07T23:51:03.509525Z","shell.execute_reply.started":"2025-10-07T23:51:02.671264Z","shell.execute_reply":"2025-10-07T23:51:03.508845Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0        17.99         10.38          122.80     1001.0          0.11840   \n1        20.57         17.77          132.90     1326.0          0.08474   \n2        19.69         21.25          130.00     1203.0          0.10960   \n3        11.42         20.38           77.58      386.1          0.14250   \n4        20.29         14.34          135.10     1297.0          0.10030   \n\n   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0           0.27760          0.3001              0.14710         0.2419   \n1           0.07864          0.0869              0.07017         0.1812   \n2           0.15990          0.1974              0.12790         0.2069   \n3           0.28390          0.2414              0.10520         0.2597   \n4           0.13280          0.1980              0.10430         0.1809   \n\n   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n0                 0.07871  ...          17.33           184.60      2019.0   \n1                 0.05667  ...          23.41           158.80      1956.0   \n2                 0.05999  ...          25.53           152.50      1709.0   \n3                 0.09744  ...          26.50            98.87       567.7   \n4                 0.05883  ...          16.67           152.20      1575.0   \n\n   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n0            0.1622             0.6656           0.7119                0.2654   \n1            0.1238             0.1866           0.2416                0.1860   \n2            0.1444             0.4245           0.4504                0.2430   \n3            0.2098             0.8663           0.6869                0.2575   \n4            0.1374             0.2050           0.4000                0.1625   \n\n   worst symmetry  worst fractal dimension  target  \n0          0.4601                  0.11890       0  \n1          0.2750                  0.08902       0  \n2          0.3613                  0.08758       0  \n3          0.6638                  0.17300       0  \n4          0.2364                  0.07678       0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:03.510500Z","iopub.execute_input":"2025-10-07T23:51:03.510855Z","iopub.status.idle":"2025-10-07T23:51:03.560280Z","shell.execute_reply.started":"2025-10-07T23:51:03.510836Z","shell.execute_reply":"2025-10-07T23:51:03.559778Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       mean radius  mean texture  mean perimeter    mean area  \\\ncount   569.000000    569.000000      569.000000   569.000000   \nmean     14.127292     19.289649       91.969033   654.889104   \nstd       3.524049      4.301036       24.298981   351.914129   \nmin       6.981000      9.710000       43.790000   143.500000   \n25%      11.700000     16.170000       75.170000   420.300000   \n50%      13.370000     18.840000       86.240000   551.100000   \n75%      15.780000     21.800000      104.100000   782.700000   \nmax      28.110000     39.280000      188.500000  2501.000000   \n\n       mean smoothness  mean compactness  mean concavity  mean concave points  \\\ncount       569.000000        569.000000      569.000000           569.000000   \nmean          0.096360          0.104341        0.088799             0.048919   \nstd           0.014064          0.052813        0.079720             0.038803   \nmin           0.052630          0.019380        0.000000             0.000000   \n25%           0.086370          0.064920        0.029560             0.020310   \n50%           0.095870          0.092630        0.061540             0.033500   \n75%           0.105300          0.130400        0.130700             0.074000   \nmax           0.163400          0.345400        0.426800             0.201200   \n\n       mean symmetry  mean fractal dimension  ...  worst texture  \\\ncount     569.000000              569.000000  ...     569.000000   \nmean        0.181162                0.062798  ...      25.677223   \nstd         0.027414                0.007060  ...       6.146258   \nmin         0.106000                0.049960  ...      12.020000   \n25%         0.161900                0.057700  ...      21.080000   \n50%         0.179200                0.061540  ...      25.410000   \n75%         0.195700                0.066120  ...      29.720000   \nmax         0.304000                0.097440  ...      49.540000   \n\n       worst perimeter   worst area  worst smoothness  worst compactness  \\\ncount       569.000000   569.000000        569.000000         569.000000   \nmean        107.261213   880.583128          0.132369           0.254265   \nstd          33.602542   569.356993          0.022832           0.157336   \nmin          50.410000   185.200000          0.071170           0.027290   \n25%          84.110000   515.300000          0.116600           0.147200   \n50%          97.660000   686.500000          0.131300           0.211900   \n75%         125.400000  1084.000000          0.146000           0.339100   \nmax         251.200000  4254.000000          0.222600           1.058000   \n\n       worst concavity  worst concave points  worst symmetry  \\\ncount       569.000000            569.000000      569.000000   \nmean          0.272188              0.114606        0.290076   \nstd           0.208624              0.065732        0.061867   \nmin           0.000000              0.000000        0.156500   \n25%           0.114500              0.064930        0.250400   \n50%           0.226700              0.099930        0.282200   \n75%           0.382900              0.161400        0.317900   \nmax           1.252000              0.291000        0.663800   \n\n       worst fractal dimension      target  \ncount               569.000000  569.000000  \nmean                  0.083946    0.627417  \nstd                   0.018061    0.483918  \nmin                   0.055040    0.000000  \n25%                   0.071460    0.000000  \n50%                   0.080040    1.000000  \n75%                   0.092080    1.000000  \nmax                   0.207500    1.000000  \n\n[8 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>0.181162</td>\n      <td>0.062798</td>\n      <td>...</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n      <td>0.627417</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>0.027414</td>\n      <td>0.007060</td>\n      <td>...</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n      <td>0.483918</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.106000</td>\n      <td>0.049960</td>\n      <td>...</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>0.161900</td>\n      <td>0.057700</td>\n      <td>...</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>0.179200</td>\n      <td>0.061540</td>\n      <td>...</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>0.195700</td>\n      <td>0.066120</td>\n      <td>...</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>0.304000</td>\n      <td>0.097440</td>\n      <td>...</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df['target'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:03.560868Z","iopub.execute_input":"2025-10-07T23:51:03.561074Z","iopub.status.idle":"2025-10-07T23:51:03.565818Z","shell.execute_reply.started":"2025-10-07T23:51:03.561057Z","shell.execute_reply":"2025-10-07T23:51:03.565221Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([0, 1])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ------------------------------\n# sklearn implementation\n# ------------------------------\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train sklearn Logistic Regression (linear classifier for binary task)\nclf = LogisticRegression(max_iter=10000)\nclf.fit(X_train, y_train)\n\nprint(\"sklearn Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:03.567378Z","iopub.execute_input":"2025-10-07T23:51:03.567642Z","iopub.status.idle":"2025-10-07T23:51:13.381883Z","shell.execute_reply.started":"2025-10-07T23:51:03.567622Z","shell.execute_reply":"2025-10-07T23:51:13.378835Z"}},"outputs":[{"name":"stdout","text":"sklearn Accuracy: 0.956140350877193\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport optax\nimport jax.numpy as jnp\nimport jax\n\n# Scale features (like sklearn would)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert to JAX\nX_train = jnp.array(X_train_scaled, dtype=jnp.float32)\ny_train = jnp.array(y_train, dtype=jnp.float32).reshape(-1, 1)\nX_test = jnp.array(X_test_scaled, dtype=jnp.float32)\ny_test = jnp.array(y_test, dtype=jnp.float32).reshape(-1, 1)\n\n# Initialize weights + bias\nkey = jax.random.PRNGKey(0)\nW = jax.random.normal(key, (X_train.shape[1], 1)) * 0.01 # Here, 0.01 is going to be the std of normal distribution it is a scaling factor and due to scaling the value itself becomes std\n\n#W is a (num_features, 1) weight matrix.\n#Initialized from ð’©(0, 0.01Â²) distribution.\n#If W = 0, every feature contributes equally at start.\n\n\nb = jnp.zeros((1,))   # Numpy/JAX broadcasting this single \nparams = (W, b)\n\n# Sigmoid\ndef sigmoid(x): return 1 / (1 + jnp.exp(-x))\n\ndef predict(params, X):\n    W, b = params\n    return sigmoid(X @ W + b)\n\n# Binary cross-entropy loss\ndef loss_fn(params, X, y):\n    y_pred = predict(params, X)\n    return -jnp.mean(y * jnp.log(y_pred + 1e-8) + (1 - y) * jnp.log(1 - y_pred + 1e-8))\n#Computes the predictions, which is the prediction probability y^ (y cap), after that BCE penalizes the predictions which are far from true_pred(y). \n# In that way, if diff between y_pred and y is larger than BCE is more which is how the loss works.\n\n# Optimizer (higher learning rate)\noptimizer = optax.sgd(learning_rate=1e-5)\nopt_state = optimizer.init(params)\n\n@jax.jit\ndef update(params, opt_state, X, y):\n    grads = jax.grad(loss_fn)(params, X, y) # computes gradients of loss w.r.t W,b\n    updates, opt_state = optimizer.update(grads, opt_state) # Opt then updates the finds the difference in parameters, updates and returns them along with a new opt state\n    new_params = optax.apply_updates(params, updates) # The new paramters are applied again \n    return new_params, opt_state\n\n# In PyTorch:\n\n# Call .backward() to compute grads (stored inside tensors).\n\n# Call optimizer.step() to apply updates (internally modifies parameters).\n\n# The state is hidden inside the optimizer object.\n\n# In JAX:\n\n# You explicitly get grads.\n\n# You explicitly compute updates.\n\n# You explicitly return (new_params, new_state).\n\n# Training loop\nfor epoch in range(10000):\n    params, opt_state = update(params, opt_state, X_train, y_train)\n\n# Evaluate\ny_pred = predict(params, X_test) > 0.5\nacc = jnp.mean(y_pred == y_test)\nprint(\"JAX Accuracy:\", float(acc))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:13.382390Z","iopub.execute_input":"2025-10-07T23:51:13.382569Z","iopub.status.idle":"2025-10-07T23:51:16.777659Z","shell.execute_reply.started":"2025-10-07T23:51:13.382554Z","shell.execute_reply":"2025-10-07T23:51:16.776941Z"}},"outputs":[{"name":"stderr","text":"INFO:2025-10-07 23:51:13,998:jax._src.xla_bridge:924: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\nINFO:2025-10-07 23:51:14,000:jax._src.xla_bridge:924: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n","output_type":"stream"},{"name":"stdout","text":"JAX Accuracy: 0.9649122953414917\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# #Training Differences:\n\n## PyTorch\n\n# # 1. Forward pass\n# y_pred = model(X)             \n    \n# # 2. Compute loss\n# loss = loss_fn(y_pred, y)    \n    \n# # 3. Zero old gradients (important!)\n# optimizer.zero_grad()          \n    \n# # 4. Backward pass (compute grads)\n# loss.backward()                \n    \n# # 5. Update parameters\n# optimizer.step()        \n\n# Gradients are stored inside each parameter (param.grad).\n\n# loss.backward() accumulates into param.grad.\n\n# zero_grad() clears old grads so accumulation doesnâ€™t mess things up.\n\n# optimizer.step() uses the stored grads to update params in-place.\n\n\n\n## JAX\n\n# # 1. Compute gradients w.r.t loss\n# grads = jax.grad(loss_fn)(params, X, y)  \n    \n# # 2. Use optimizer to compute updates + new opt state\n# updates, opt_state = optimizer.update(grads, opt_state)  \n    \n# # 3. Apply updates to parameters\n# params = optax.apply_updates(params, updates)  \n\n# No zero_grad() â€” grads donâ€™t accumulate, theyâ€™re computed fresh each step.\n\n# opt_state is explicitly passed around to track optimizer history\n\n# Params are immutable â†’ we return new params each step.\n\n# Functional style: each function call = new state, no side effects & with JIT faster runtime.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:16.778444Z","iopub.execute_input":"2025-10-07T23:51:16.778899Z","iopub.status.idle":"2025-10-07T23:51:16.783051Z","shell.execute_reply.started":"2025-10-07T23:51:16.778876Z","shell.execute_reply":"2025-10-07T23:51:16.782431Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# MLP for Binary Classification","metadata":{}},{"cell_type":"code","source":"X_train.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:16.783825Z","iopub.execute_input":"2025-10-07T23:51:16.784631Z","iopub.status.idle":"2025-10-07T23:51:16.806514Z","shell.execute_reply.started":"2025-10-07T23:51:16.784612Z","shell.execute_reply":"2025-10-07T23:51:16.805993Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load and preprocess dataset\nX, y = load_breast_cancer(return_X_y=True)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_torch = torch.tensor(X_train, dtype=torch.float32)\ny_train_torch = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_test_torch = torch.tensor(X_test, dtype=torch.float32)\ny_test_torch = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# Define MLP\nclass MLP(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.fc2 = nn.Linear(64, 32) \n        self.fc3 = nn.Linear(32, 1)\n    def forward(self, x):\n        x = torch.relu(self.fc1(x)) # ReLU for adding non-linearity\n        x = torch.relu(self.fc2(x))\n        return torch.sigmoid(self.fc3(x))\n\n# Initialize model, loss, optimizer\nmodel = MLP(X_train.shape[1])\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\n\n# Training loop\nnum_epochs = 256\nbatch_size = 64\n\nfor epoch in range(num_epochs):\n    perm = torch.randperm(X_train_torch.size(0)) #Random permutations so that each epoch get different batches which helps the model to learn and converge quickly\n    total_loss = 0\n    for i in range(0, X_train_torch.size(0), batch_size):\n        idx = perm[i:i+batch_size]\n        xb, yb = X_train_torch[idx], y_train_torch[idx]\n        pred = model(xb)\n        loss = criterion(pred, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * len(xb)\n    avg_loss = total_loss / len(X_train_torch)\n    \n    # Evaluate each epoch\n    with torch.no_grad():\n        preds = (model(X_test_torch) > 0.5).float()\n        acc = (preds.eq(y_test_torch)).float().mean().item()\n    print(f\"Epoch [{epoch+1}/{num_epochs}]  Loss: {avg_loss:.4f}  Test Accuracy: {acc*100:.2f}%\")\n\n# Final evaluation\nwith torch.no_grad():\n    preds = (model(X_test_torch) > 0.5).float()\n    test_acc = (preds.eq(y_test_torch)).float().mean().item()\n\nprint(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:16.807176Z","iopub.execute_input":"2025-10-07T23:51:16.807411Z","iopub.status.idle":"2025-10-07T23:51:25.574084Z","shell.execute_reply.started":"2025-10-07T23:51:16.807388Z","shell.execute_reply":"2025-10-07T23:51:25.573262Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/256]  Loss: 0.6839  Test Accuracy: 63.16%\nEpoch [2/256]  Loss: 0.6831  Test Accuracy: 63.16%\nEpoch [3/256]  Loss: 0.6823  Test Accuracy: 63.16%\nEpoch [4/256]  Loss: 0.6815  Test Accuracy: 64.04%\nEpoch [5/256]  Loss: 0.6807  Test Accuracy: 64.91%\nEpoch [6/256]  Loss: 0.6800  Test Accuracy: 64.91%\nEpoch [7/256]  Loss: 0.6792  Test Accuracy: 64.91%\nEpoch [8/256]  Loss: 0.6784  Test Accuracy: 65.79%\nEpoch [9/256]  Loss: 0.6777  Test Accuracy: 65.79%\nEpoch [10/256]  Loss: 0.6769  Test Accuracy: 65.79%\nEpoch [11/256]  Loss: 0.6761  Test Accuracy: 65.79%\nEpoch [12/256]  Loss: 0.6753  Test Accuracy: 65.79%\nEpoch [13/256]  Loss: 0.6745  Test Accuracy: 65.79%\nEpoch [14/256]  Loss: 0.6738  Test Accuracy: 65.79%\nEpoch [15/256]  Loss: 0.6730  Test Accuracy: 65.79%\nEpoch [16/256]  Loss: 0.6722  Test Accuracy: 65.79%\nEpoch [17/256]  Loss: 0.6714  Test Accuracy: 65.79%\nEpoch [18/256]  Loss: 0.6706  Test Accuracy: 65.79%\nEpoch [19/256]  Loss: 0.6698  Test Accuracy: 65.79%\nEpoch [20/256]  Loss: 0.6690  Test Accuracy: 67.54%\nEpoch [21/256]  Loss: 0.6683  Test Accuracy: 67.54%\nEpoch [22/256]  Loss: 0.6675  Test Accuracy: 68.42%\nEpoch [23/256]  Loss: 0.6667  Test Accuracy: 68.42%\nEpoch [24/256]  Loss: 0.6660  Test Accuracy: 68.42%\nEpoch [25/256]  Loss: 0.6652  Test Accuracy: 69.30%\nEpoch [26/256]  Loss: 0.6644  Test Accuracy: 70.18%\nEpoch [27/256]  Loss: 0.6636  Test Accuracy: 70.18%\nEpoch [28/256]  Loss: 0.6628  Test Accuracy: 70.18%\nEpoch [29/256]  Loss: 0.6620  Test Accuracy: 70.18%\nEpoch [30/256]  Loss: 0.6612  Test Accuracy: 71.93%\nEpoch [31/256]  Loss: 0.6604  Test Accuracy: 72.81%\nEpoch [32/256]  Loss: 0.6596  Test Accuracy: 72.81%\nEpoch [33/256]  Loss: 0.6588  Test Accuracy: 72.81%\nEpoch [34/256]  Loss: 0.6579  Test Accuracy: 72.81%\nEpoch [35/256]  Loss: 0.6571  Test Accuracy: 72.81%\nEpoch [36/256]  Loss: 0.6563  Test Accuracy: 72.81%\nEpoch [37/256]  Loss: 0.6555  Test Accuracy: 72.81%\nEpoch [38/256]  Loss: 0.6547  Test Accuracy: 73.68%\nEpoch [39/256]  Loss: 0.6539  Test Accuracy: 73.68%\nEpoch [40/256]  Loss: 0.6531  Test Accuracy: 75.44%\nEpoch [41/256]  Loss: 0.6522  Test Accuracy: 77.19%\nEpoch [42/256]  Loss: 0.6514  Test Accuracy: 77.19%\nEpoch [43/256]  Loss: 0.6506  Test Accuracy: 78.07%\nEpoch [44/256]  Loss: 0.6497  Test Accuracy: 78.07%\nEpoch [45/256]  Loss: 0.6489  Test Accuracy: 78.07%\nEpoch [46/256]  Loss: 0.6481  Test Accuracy: 78.07%\nEpoch [47/256]  Loss: 0.6472  Test Accuracy: 78.95%\nEpoch [48/256]  Loss: 0.6464  Test Accuracy: 79.82%\nEpoch [49/256]  Loss: 0.6455  Test Accuracy: 79.82%\nEpoch [50/256]  Loss: 0.6447  Test Accuracy: 80.70%\nEpoch [51/256]  Loss: 0.6438  Test Accuracy: 81.58%\nEpoch [52/256]  Loss: 0.6430  Test Accuracy: 82.46%\nEpoch [53/256]  Loss: 0.6421  Test Accuracy: 82.46%\nEpoch [54/256]  Loss: 0.6412  Test Accuracy: 82.46%\nEpoch [55/256]  Loss: 0.6404  Test Accuracy: 82.46%\nEpoch [56/256]  Loss: 0.6395  Test Accuracy: 82.46%\nEpoch [57/256]  Loss: 0.6386  Test Accuracy: 83.33%\nEpoch [58/256]  Loss: 0.6378  Test Accuracy: 83.33%\nEpoch [59/256]  Loss: 0.6369  Test Accuracy: 83.33%\nEpoch [60/256]  Loss: 0.6360  Test Accuracy: 83.33%\nEpoch [61/256]  Loss: 0.6351  Test Accuracy: 83.33%\nEpoch [62/256]  Loss: 0.6342  Test Accuracy: 83.33%\nEpoch [63/256]  Loss: 0.6333  Test Accuracy: 84.21%\nEpoch [64/256]  Loss: 0.6324  Test Accuracy: 84.21%\nEpoch [65/256]  Loss: 0.6316  Test Accuracy: 84.21%\nEpoch [66/256]  Loss: 0.6307  Test Accuracy: 84.21%\nEpoch [67/256]  Loss: 0.6298  Test Accuracy: 84.21%\nEpoch [68/256]  Loss: 0.6289  Test Accuracy: 84.21%\nEpoch [69/256]  Loss: 0.6280  Test Accuracy: 84.21%\nEpoch [70/256]  Loss: 0.6271  Test Accuracy: 84.21%\nEpoch [71/256]  Loss: 0.6262  Test Accuracy: 84.21%\nEpoch [72/256]  Loss: 0.6252  Test Accuracy: 85.09%\nEpoch [73/256]  Loss: 0.6243  Test Accuracy: 85.09%\nEpoch [74/256]  Loss: 0.6234  Test Accuracy: 85.09%\nEpoch [75/256]  Loss: 0.6225  Test Accuracy: 85.09%\nEpoch [76/256]  Loss: 0.6215  Test Accuracy: 85.96%\nEpoch [77/256]  Loss: 0.6206  Test Accuracy: 85.96%\nEpoch [78/256]  Loss: 0.6197  Test Accuracy: 85.96%\nEpoch [79/256]  Loss: 0.6188  Test Accuracy: 85.96%\nEpoch [80/256]  Loss: 0.6179  Test Accuracy: 86.84%\nEpoch [81/256]  Loss: 0.6169  Test Accuracy: 86.84%\nEpoch [82/256]  Loss: 0.6160  Test Accuracy: 86.84%\nEpoch [83/256]  Loss: 0.6150  Test Accuracy: 86.84%\nEpoch [84/256]  Loss: 0.6140  Test Accuracy: 86.84%\nEpoch [85/256]  Loss: 0.6131  Test Accuracy: 86.84%\nEpoch [86/256]  Loss: 0.6121  Test Accuracy: 86.84%\nEpoch [87/256]  Loss: 0.6111  Test Accuracy: 86.84%\nEpoch [88/256]  Loss: 0.6101  Test Accuracy: 86.84%\nEpoch [89/256]  Loss: 0.6091  Test Accuracy: 86.84%\nEpoch [90/256]  Loss: 0.6082  Test Accuracy: 86.84%\nEpoch [91/256]  Loss: 0.6072  Test Accuracy: 87.72%\nEpoch [92/256]  Loss: 0.6062  Test Accuracy: 87.72%\nEpoch [93/256]  Loss: 0.6053  Test Accuracy: 87.72%\nEpoch [94/256]  Loss: 0.6043  Test Accuracy: 88.60%\nEpoch [95/256]  Loss: 0.6033  Test Accuracy: 88.60%\nEpoch [96/256]  Loss: 0.6023  Test Accuracy: 88.60%\nEpoch [97/256]  Loss: 0.6013  Test Accuracy: 88.60%\nEpoch [98/256]  Loss: 0.6003  Test Accuracy: 88.60%\nEpoch [99/256]  Loss: 0.5993  Test Accuracy: 88.60%\nEpoch [100/256]  Loss: 0.5982  Test Accuracy: 88.60%\nEpoch [101/256]  Loss: 0.5972  Test Accuracy: 88.60%\nEpoch [102/256]  Loss: 0.5962  Test Accuracy: 88.60%\nEpoch [103/256]  Loss: 0.5951  Test Accuracy: 88.60%\nEpoch [104/256]  Loss: 0.5941  Test Accuracy: 88.60%\nEpoch [105/256]  Loss: 0.5930  Test Accuracy: 88.60%\nEpoch [106/256]  Loss: 0.5920  Test Accuracy: 89.47%\nEpoch [107/256]  Loss: 0.5909  Test Accuracy: 90.35%\nEpoch [108/256]  Loss: 0.5898  Test Accuracy: 90.35%\nEpoch [109/256]  Loss: 0.5888  Test Accuracy: 90.35%\nEpoch [110/256]  Loss: 0.5877  Test Accuracy: 91.23%\nEpoch [111/256]  Loss: 0.5866  Test Accuracy: 91.23%\nEpoch [112/256]  Loss: 0.5856  Test Accuracy: 91.23%\nEpoch [113/256]  Loss: 0.5845  Test Accuracy: 91.23%\nEpoch [114/256]  Loss: 0.5835  Test Accuracy: 90.35%\nEpoch [115/256]  Loss: 0.5824  Test Accuracy: 90.35%\nEpoch [116/256]  Loss: 0.5813  Test Accuracy: 90.35%\nEpoch [117/256]  Loss: 0.5802  Test Accuracy: 90.35%\nEpoch [118/256]  Loss: 0.5791  Test Accuracy: 90.35%\nEpoch [119/256]  Loss: 0.5780  Test Accuracy: 91.23%\nEpoch [120/256]  Loss: 0.5770  Test Accuracy: 91.23%\nEpoch [121/256]  Loss: 0.5759  Test Accuracy: 92.11%\nEpoch [122/256]  Loss: 0.5748  Test Accuracy: 92.11%\nEpoch [123/256]  Loss: 0.5737  Test Accuracy: 92.11%\nEpoch [124/256]  Loss: 0.5726  Test Accuracy: 92.11%\nEpoch [125/256]  Loss: 0.5715  Test Accuracy: 92.11%\nEpoch [126/256]  Loss: 0.5704  Test Accuracy: 92.11%\nEpoch [127/256]  Loss: 0.5693  Test Accuracy: 92.11%\nEpoch [128/256]  Loss: 0.5682  Test Accuracy: 92.11%\nEpoch [129/256]  Loss: 0.5671  Test Accuracy: 92.11%\nEpoch [130/256]  Loss: 0.5659  Test Accuracy: 92.11%\nEpoch [131/256]  Loss: 0.5648  Test Accuracy: 92.11%\nEpoch [132/256]  Loss: 0.5636  Test Accuracy: 92.11%\nEpoch [133/256]  Loss: 0.5624  Test Accuracy: 92.11%\nEpoch [134/256]  Loss: 0.5613  Test Accuracy: 92.11%\nEpoch [135/256]  Loss: 0.5602  Test Accuracy: 92.11%\nEpoch [136/256]  Loss: 0.5590  Test Accuracy: 92.11%\nEpoch [137/256]  Loss: 0.5579  Test Accuracy: 92.11%\nEpoch [138/256]  Loss: 0.5567  Test Accuracy: 92.98%\nEpoch [139/256]  Loss: 0.5556  Test Accuracy: 92.98%\nEpoch [140/256]  Loss: 0.5544  Test Accuracy: 92.98%\nEpoch [141/256]  Loss: 0.5533  Test Accuracy: 92.98%\nEpoch [142/256]  Loss: 0.5522  Test Accuracy: 92.98%\nEpoch [143/256]  Loss: 0.5511  Test Accuracy: 92.98%\nEpoch [144/256]  Loss: 0.5499  Test Accuracy: 92.98%\nEpoch [145/256]  Loss: 0.5488  Test Accuracy: 92.98%\nEpoch [146/256]  Loss: 0.5476  Test Accuracy: 93.86%\nEpoch [147/256]  Loss: 0.5465  Test Accuracy: 93.86%\nEpoch [148/256]  Loss: 0.5454  Test Accuracy: 93.86%\nEpoch [149/256]  Loss: 0.5442  Test Accuracy: 93.86%\nEpoch [150/256]  Loss: 0.5431  Test Accuracy: 93.86%\nEpoch [151/256]  Loss: 0.5419  Test Accuracy: 93.86%\nEpoch [152/256]  Loss: 0.5407  Test Accuracy: 93.86%\nEpoch [153/256]  Loss: 0.5395  Test Accuracy: 93.86%\nEpoch [154/256]  Loss: 0.5384  Test Accuracy: 93.86%\nEpoch [155/256]  Loss: 0.5372  Test Accuracy: 93.86%\nEpoch [156/256]  Loss: 0.5360  Test Accuracy: 93.86%\nEpoch [157/256]  Loss: 0.5348  Test Accuracy: 93.86%\nEpoch [158/256]  Loss: 0.5336  Test Accuracy: 93.86%\nEpoch [159/256]  Loss: 0.5325  Test Accuracy: 93.86%\nEpoch [160/256]  Loss: 0.5313  Test Accuracy: 93.86%\nEpoch [161/256]  Loss: 0.5301  Test Accuracy: 93.86%\nEpoch [162/256]  Loss: 0.5289  Test Accuracy: 93.86%\nEpoch [163/256]  Loss: 0.5277  Test Accuracy: 93.86%\nEpoch [164/256]  Loss: 0.5265  Test Accuracy: 93.86%\nEpoch [165/256]  Loss: 0.5253  Test Accuracy: 93.86%\nEpoch [166/256]  Loss: 0.5241  Test Accuracy: 93.86%\nEpoch [167/256]  Loss: 0.5229  Test Accuracy: 93.86%\nEpoch [168/256]  Loss: 0.5217  Test Accuracy: 93.86%\nEpoch [169/256]  Loss: 0.5205  Test Accuracy: 93.86%\nEpoch [170/256]  Loss: 0.5193  Test Accuracy: 93.86%\nEpoch [171/256]  Loss: 0.5181  Test Accuracy: 93.86%\nEpoch [172/256]  Loss: 0.5169  Test Accuracy: 93.86%\nEpoch [173/256]  Loss: 0.5156  Test Accuracy: 93.86%\nEpoch [174/256]  Loss: 0.5144  Test Accuracy: 93.86%\nEpoch [175/256]  Loss: 0.5132  Test Accuracy: 93.86%\nEpoch [176/256]  Loss: 0.5120  Test Accuracy: 93.86%\nEpoch [177/256]  Loss: 0.5108  Test Accuracy: 93.86%\nEpoch [178/256]  Loss: 0.5095  Test Accuracy: 93.86%\nEpoch [179/256]  Loss: 0.5083  Test Accuracy: 93.86%\nEpoch [180/256]  Loss: 0.5070  Test Accuracy: 93.86%\nEpoch [181/256]  Loss: 0.5058  Test Accuracy: 94.74%\nEpoch [182/256]  Loss: 0.5046  Test Accuracy: 94.74%\nEpoch [183/256]  Loss: 0.5034  Test Accuracy: 94.74%\nEpoch [184/256]  Loss: 0.5021  Test Accuracy: 94.74%\nEpoch [185/256]  Loss: 0.5009  Test Accuracy: 94.74%\nEpoch [186/256]  Loss: 0.4997  Test Accuracy: 94.74%\nEpoch [187/256]  Loss: 0.4985  Test Accuracy: 94.74%\nEpoch [188/256]  Loss: 0.4973  Test Accuracy: 94.74%\nEpoch [189/256]  Loss: 0.4961  Test Accuracy: 94.74%\nEpoch [190/256]  Loss: 0.4948  Test Accuracy: 94.74%\nEpoch [191/256]  Loss: 0.4936  Test Accuracy: 94.74%\nEpoch [192/256]  Loss: 0.4924  Test Accuracy: 94.74%\nEpoch [193/256]  Loss: 0.4912  Test Accuracy: 94.74%\nEpoch [194/256]  Loss: 0.4900  Test Accuracy: 94.74%\nEpoch [195/256]  Loss: 0.4888  Test Accuracy: 94.74%\nEpoch [196/256]  Loss: 0.4875  Test Accuracy: 94.74%\nEpoch [197/256]  Loss: 0.4863  Test Accuracy: 94.74%\nEpoch [198/256]  Loss: 0.4851  Test Accuracy: 94.74%\nEpoch [199/256]  Loss: 0.4839  Test Accuracy: 94.74%\nEpoch [200/256]  Loss: 0.4827  Test Accuracy: 94.74%\nEpoch [201/256]  Loss: 0.4814  Test Accuracy: 94.74%\nEpoch [202/256]  Loss: 0.4802  Test Accuracy: 94.74%\nEpoch [203/256]  Loss: 0.4789  Test Accuracy: 94.74%\nEpoch [204/256]  Loss: 0.4777  Test Accuracy: 94.74%\nEpoch [205/256]  Loss: 0.4764  Test Accuracy: 94.74%\nEpoch [206/256]  Loss: 0.4752  Test Accuracy: 94.74%\nEpoch [207/256]  Loss: 0.4739  Test Accuracy: 94.74%\nEpoch [208/256]  Loss: 0.4727  Test Accuracy: 94.74%\nEpoch [209/256]  Loss: 0.4715  Test Accuracy: 94.74%\nEpoch [210/256]  Loss: 0.4702  Test Accuracy: 94.74%\nEpoch [211/256]  Loss: 0.4690  Test Accuracy: 94.74%\nEpoch [212/256]  Loss: 0.4678  Test Accuracy: 95.61%\nEpoch [213/256]  Loss: 0.4666  Test Accuracy: 95.61%\nEpoch [214/256]  Loss: 0.4653  Test Accuracy: 95.61%\nEpoch [215/256]  Loss: 0.4641  Test Accuracy: 95.61%\nEpoch [216/256]  Loss: 0.4628  Test Accuracy: 95.61%\nEpoch [217/256]  Loss: 0.4616  Test Accuracy: 95.61%\nEpoch [218/256]  Loss: 0.4604  Test Accuracy: 95.61%\nEpoch [219/256]  Loss: 0.4591  Test Accuracy: 95.61%\nEpoch [220/256]  Loss: 0.4580  Test Accuracy: 95.61%\nEpoch [221/256]  Loss: 0.4567  Test Accuracy: 95.61%\nEpoch [222/256]  Loss: 0.4555  Test Accuracy: 95.61%\nEpoch [223/256]  Loss: 0.4543  Test Accuracy: 95.61%\nEpoch [224/256]  Loss: 0.4531  Test Accuracy: 95.61%\nEpoch [225/256]  Loss: 0.4518  Test Accuracy: 95.61%\nEpoch [226/256]  Loss: 0.4506  Test Accuracy: 95.61%\nEpoch [227/256]  Loss: 0.4493  Test Accuracy: 95.61%\nEpoch [228/256]  Loss: 0.4481  Test Accuracy: 95.61%\nEpoch [229/256]  Loss: 0.4468  Test Accuracy: 95.61%\nEpoch [230/256]  Loss: 0.4456  Test Accuracy: 95.61%\nEpoch [231/256]  Loss: 0.4444  Test Accuracy: 95.61%\nEpoch [232/256]  Loss: 0.4432  Test Accuracy: 95.61%\nEpoch [233/256]  Loss: 0.4420  Test Accuracy: 95.61%\nEpoch [234/256]  Loss: 0.4407  Test Accuracy: 95.61%\nEpoch [235/256]  Loss: 0.4395  Test Accuracy: 95.61%\nEpoch [236/256]  Loss: 0.4383  Test Accuracy: 95.61%\nEpoch [237/256]  Loss: 0.4371  Test Accuracy: 95.61%\nEpoch [238/256]  Loss: 0.4359  Test Accuracy: 95.61%\nEpoch [239/256]  Loss: 0.4347  Test Accuracy: 95.61%\nEpoch [240/256]  Loss: 0.4335  Test Accuracy: 95.61%\nEpoch [241/256]  Loss: 0.4323  Test Accuracy: 95.61%\nEpoch [242/256]  Loss: 0.4311  Test Accuracy: 95.61%\nEpoch [243/256]  Loss: 0.4299  Test Accuracy: 95.61%\nEpoch [244/256]  Loss: 0.4287  Test Accuracy: 95.61%\nEpoch [245/256]  Loss: 0.4275  Test Accuracy: 95.61%\nEpoch [246/256]  Loss: 0.4264  Test Accuracy: 95.61%\nEpoch [247/256]  Loss: 0.4252  Test Accuracy: 95.61%\nEpoch [248/256]  Loss: 0.4240  Test Accuracy: 95.61%\nEpoch [249/256]  Loss: 0.4228  Test Accuracy: 95.61%\nEpoch [250/256]  Loss: 0.4216  Test Accuracy: 95.61%\nEpoch [251/256]  Loss: 0.4204  Test Accuracy: 95.61%\nEpoch [252/256]  Loss: 0.4192  Test Accuracy: 95.61%\nEpoch [253/256]  Loss: 0.4180  Test Accuracy: 95.61%\nEpoch [254/256]  Loss: 0.4168  Test Accuracy: 95.61%\nEpoch [255/256]  Loss: 0.4156  Test Accuracy: 95.61%\nEpoch [256/256]  Loss: 0.4145  Test Accuracy: 95.61%\n\nFinal Test Accuracy: 95.61%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport optax\nimport flax.linen as nn\nfrom flax.training import train_state\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n# ------------------------------------------------------------\n# Load and preprocess dataset\n# ------------------------------------------------------------\nX, y = load_breast_cancer(return_X_y=True)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train = jnp.array(X_train, dtype=jnp.float32)\ny_train = jnp.array(y_train, dtype=jnp.float32).reshape(-1, 1)\nX_test = jnp.array(X_test, dtype=jnp.float32)\ny_test = jnp.array(y_test, dtype=jnp.float32).reshape(-1, 1)\n\n# ------------------------------------------------------------\n# Define MLP model (64 -> 32 -> 1)\n# ------------------------------------------------------------\nclass MLP(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.relu(nn.Dense(64)(x))\n        x = nn.relu(nn.Dense(32)(x))\n        x = nn.Dense(1)(x)\n        return nn.sigmoid(x)\n\n# ------------------------------------------------------------\n# Initialize model and optimizer\n# ------------------------------------------------------------\nmodel = MLP()\nrng = jax.random.PRNGKey(0)\nparams = model.init(rng, X_train) # model.init() requires key and x_train so that it will automatically calls the shape and initializes the internal parameter iniatialization which is done explicitly in linear regression example.\n\nstate = train_state.TrainState.create(\n    apply_fn=model.apply,\n    params=params,\n    tx=optax.adam(1e-5)   # keep learning rate 1e-5\n)\n\n# ------------------------------------------------------------\n# Loss and accuracy functions\n# ------------------------------------------------------------\ndef loss_fn(params, X, y):\n    preds = model.apply(params, X)\n    return jnp.mean(optax.sigmoid_binary_cross_entropy(preds, y))\n\ndef accuracy_fn(params, X, y):\n    preds = model.apply(params, X)\n    preds_binary = preds > 0.5\n    return jnp.mean(preds_binary == y)\n\n# ------------------------------------------------------------\n# Training step (JIT compiled)\n# ------------------------------------------------------------\n@jax.jit\ndef train_step(state, X, y):\n    grads = jax.grad(loss_fn)(state.params, X, y)\n    state = state.apply_gradients(grads=grads)\n    loss = loss_fn(state.params, X, y)\n    acc = accuracy_fn(state.params, X, y)\n    return state, loss, acc\n\n# ------------------------------------------------------------\n# Data loader for batching\n# ------------------------------------------------------------\ndef data_loader(X, y, batch_size, rng):\n    n = X.shape[0]\n    perm = jax.random.permutation(rng, n)\n    for i in range(0, n, batch_size):\n        idx = perm[i:i+batch_size]\n        yield X[idx], y[idx]\n\n# ------------------------------------------------------------\n# Training loop\n# ------------------------------------------------------------\nepochs = 256\nbatch_size = 64\nfor epoch in range(epochs):\n    rng, input_rng = jax.random.split(rng)\n    epoch_losses, epoch_accs = [], []\n    \n    for X_batch, y_batch in data_loader(X_train, y_train, batch_size, input_rng):\n        state, loss_val, acc_val = train_step(state, X_batch, y_batch)\n        epoch_losses.append(loss_val)\n        epoch_accs.append(acc_val)\n    \n    mean_loss = jnp.mean(jnp.array(epoch_losses))\n    mean_acc = jnp.mean(jnp.array(epoch_accs))\n    \n    if (epoch + 1) % 10 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1:03d} | Loss: {mean_loss:.4f} | Train Acc: {mean_acc:.4f}\")\n\n# ------------------------------------------------------------\n# Final test accuracy\n# ------------------------------------------------------------\ntest_preds = model.apply(state.params, X_test) > 0.5\ntest_acc = jnp.mean(test_preds == y_test)\nprint(\"\\nFinal Test Accuracy:\", float(test_acc))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:25.574942Z","iopub.execute_input":"2025-10-07T23:51:25.575349Z","iopub.status.idle":"2025-10-07T23:51:39.959090Z","shell.execute_reply.started":"2025-10-07T23:51:25.575329Z","shell.execute_reply":"2025-10-07T23:51:39.958268Z"}},"outputs":[{"name":"stdout","text":"Epoch 001 | Loss: 0.6331 | Train Acc: 0.7065\nEpoch 010 | Loss: 0.6179 | Train Acc: 0.7517\nEpoch 020 | Loss: 0.6402 | Train Acc: 0.7453\nEpoch 030 | Loss: 0.6188 | Train Acc: 0.7729\nEpoch 040 | Loss: 0.6292 | Train Acc: 0.8203\nEpoch 050 | Loss: 0.5979 | Train Acc: 0.8359\nEpoch 060 | Loss: 0.5918 | Train Acc: 0.8477\nEpoch 070 | Loss: 0.5867 | Train Acc: 0.8594\nEpoch 080 | Loss: 0.5849 | Train Acc: 0.8750\nEpoch 090 | Loss: 0.5912 | Train Acc: 0.8848\nEpoch 100 | Loss: 0.5842 | Train Acc: 0.8926\nEpoch 110 | Loss: 0.5810 | Train Acc: 0.9004\nEpoch 120 | Loss: 0.5831 | Train Acc: 0.9062\nEpoch 130 | Loss: 0.5681 | Train Acc: 0.8943\nEpoch 140 | Loss: 0.5591 | Train Acc: 0.9121\nEpoch 150 | Loss: 0.5816 | Train Acc: 0.9199\nEpoch 160 | Loss: 0.5677 | Train Acc: 0.8761\nEpoch 170 | Loss: 0.5656 | Train Acc: 0.9277\nEpoch 180 | Loss: 0.5630 | Train Acc: 0.9316\nEpoch 190 | Loss: 0.5627 | Train Acc: 0.9216\nEpoch 200 | Loss: 0.5634 | Train Acc: 0.9235\nEpoch 210 | Loss: 0.5620 | Train Acc: 0.9414\nEpoch 220 | Loss: 0.5502 | Train Acc: 0.9116\nEpoch 230 | Loss: 0.5566 | Train Acc: 0.9473\nEpoch 240 | Loss: 0.5391 | Train Acc: 0.9512\nEpoch 250 | Loss: 0.5281 | Train Acc: 0.9551\n\nFinal Test Accuracy: 0.9649122953414917\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CNN for MNIST","metadata":{}},{"cell_type":"code","source":"# ------------------------------\n# PyTorch CNN on MNIST (CUDA optimized for Kaggle)\n# ------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Dataset\ntransform = transforms.ToTensor()\ntrain_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\ntest_data = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=1000)\n\n# Define CNN\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.fc1 = nn.Linear(26*26*32, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.flatten(x, 1)\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\n# Initialize model, loss, and optimizer\nmodel = CNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\n\n# Training loop\nepochs = 16\nfor epoch in range(epochs):\n    model.train()\n    correct = 0\n    total = 0\n    running_loss = 0.0\n\n    for xb, yb in train_loader:\n        xb, yb = xb.to(device), yb.to(device)\n\n        pred = model(xb)\n        loss = criterion(pred, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * xb.size(0)\n        correct += (pred.argmax(dim=1) == yb).sum().item()\n        total += yb.size(0)\n\n    train_acc = correct / total\n    avg_loss = running_loss / total\n    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n\n# Evaluation on test data\nmodel.eval()\ncorrect = 0\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb, yb = xb.to(device), yb.to(device)\n        pred = model(xb).argmax(dim=1)\n        correct += pred.eq(yb).sum().item()\n\ntest_acc = correct / len(test_data)\nprint(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:51:39.978038Z","iopub.execute_input":"2025-10-07T23:51:39.978266Z","iopub.status.idle":"2025-10-07T23:53:23.378843Z","shell.execute_reply.started":"2025-10-07T23:51:39.978241Z","shell.execute_reply":"2025-10-07T23:53:23.378026Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1/16 | Loss: 1.5009 | Train Acc: 74.42%\nEpoch 2/16 | Loss: 0.7410 | Train Acc: 85.61%\nEpoch 3/16 | Loss: 0.5176 | Train Acc: 88.06%\nEpoch 4/16 | Loss: 0.4219 | Train Acc: 89.39%\nEpoch 5/16 | Loss: 0.3693 | Train Acc: 90.16%\nEpoch 6/16 | Loss: 0.3350 | Train Acc: 90.81%\nEpoch 7/16 | Loss: 0.3107 | Train Acc: 91.32%\nEpoch 8/16 | Loss: 0.2912 | Train Acc: 91.84%\nEpoch 9/16 | Loss: 0.2755 | Train Acc: 92.21%\nEpoch 10/16 | Loss: 0.2617 | Train Acc: 92.65%\nEpoch 11/16 | Loss: 0.2499 | Train Acc: 92.95%\nEpoch 12/16 | Loss: 0.2393 | Train Acc: 93.24%\nEpoch 13/16 | Loss: 0.2299 | Train Acc: 93.50%\nEpoch 14/16 | Loss: 0.2214 | Train Acc: 93.74%\nEpoch 15/16 | Loss: 0.2133 | Train Acc: 93.95%\nEpoch 16/16 | Loss: 0.2057 | Train Acc: 94.22%\n\nFinal Test Accuracy: 94.41%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#No need to manually tell the code to use GPU device it does it automatically.\n\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom flax.training import train_state\nimport optax\nimport tensorflow_datasets as tfds\n\n# Dataset\nds_builder = tfds.builder(\"mnist\")\nds_builder.download_and_prepare()\ntrain_ds = ds_builder.as_dataset(split=\"train\").batch(64).prefetch(1)\ntest_ds = ds_builder.as_dataset(split=\"test\").batch(1000)\n\ndef preprocess(batch):\n    images = jnp.array(batch[\"image\"], dtype=jnp.float32) / 255.0\n    labels = jnp.array(batch[\"label\"], dtype=jnp.int32)\n    return images, labels\n\n# Define CNN\nclass CNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(32, (3,3))(x)\n        x = nn.relu(x)\n        x = x.reshape((x.shape[0], -1))\n        x = nn.relu(nn.Dense(128)(x))\n        x = nn.Dense(10)(x)\n        return x\n\n# Init model\nmodel = CNN()\nparams = model.init(jax.random.PRNGKey(0), jnp.ones((1,28,28,1)))\nstate = train_state.TrainState.create(\n    apply_fn=model.apply,\n    params=params,\n    tx=optax.adam(1e-5)\n)\n\n# Loss and accuracy functions\ndef loss_fn(params, X, y):\n    logits = model.apply(params, X)\n    return optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n\ndef accuracy_fn(params, X, y):\n    logits = model.apply(params, X)\n    preds = jnp.argmax(logits, axis=1)\n    return jnp.mean(preds == y)\n\n@jax.jit\ndef train_step(state, X, y):\n    grads = jax.grad(loss_fn)(state.params, X, y)\n    state = state.apply_gradients(grads=grads)\n    loss = loss_fn(state.params, X, y)\n    acc = accuracy_fn(state.params, X, y)\n    return state, loss, acc\n\n# Training loop\nfor epoch in range(16):\n    train_loss, train_acc = 0.0, 0.0\n    num_batches = 0\n    for batch in train_ds:\n        X, y = preprocess(batch)\n        state, loss_val, acc_val = train_step(state, X, y)\n        train_loss += loss_val\n        train_acc += acc_val\n        num_batches += 1\n    train_loss /= num_batches\n    train_acc /= num_batches\n\n    # Evaluate on test set\n    correct, total = 0, 0\n    for batch in test_ds:\n        X, y = preprocess(batch)\n        logits = model.apply(state.params, X)\n        preds = jnp.argmax(logits, axis=1)\n        correct += (preds == y).sum()\n        total += len(y)\n    test_acc = correct / total\n\n    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {float(test_acc):.4f}\")\n\n## apply_gradients is for updating gradient which does both forward n backward propagation and apply is for updating the opt state it only does forward pass not the backward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T23:53:23.379802Z","iopub.execute_input":"2025-10-07T23:53:23.380330Z","iopub.status.idle":"2025-10-07T23:55:08.922574Z","shell.execute_reply.started":"2025-10-07T23:53:23.380305Z","shell.execute_reply":"2025-10-07T23:55:08.921788Z"}},"outputs":[{"name":"stdout","text":"Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/mnist/3.0.1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320256c3ebcd4f26b44f1d47bc82e0ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbc5b03d004a4f05b5225f5e4e743ba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d637f3dd214925a580aac95b18d37f"}},"metadata":{}},{"name":"stderr","text":"2025-10-07 23:53:27.873815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759881208.070627     306 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759881208.135063     306 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1759881217.070578     110 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2967 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/mnist/incomplete.GI32C6_3.0.1/mnist-train.tfrecord*...:   0%|          | 0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/mnist/incomplete.GI32C6_3.0.1/mnist-test.tfrecord*...:   0%|          | 0/â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\nEpoch 01 | Train Loss: 0.7114 | Train Acc: 0.8497 | Test Acc: 0.9174\nEpoch 02 | Train Loss: 0.2998 | Train Acc: 0.9222 | Test Acc: 0.9337\nEpoch 03 | Train Loss: 0.2356 | Train Acc: 0.9371 | Test Acc: 0.9432\nEpoch 04 | Train Loss: 0.2001 | Train Acc: 0.9459 | Test Acc: 0.9486\nEpoch 05 | Train Loss: 0.1752 | Train Acc: 0.9522 | Test Acc: 0.9541\nEpoch 06 | Train Loss: 0.1559 | Train Acc: 0.9575 | Test Acc: 0.9582\nEpoch 07 | Train Loss: 0.1403 | Train Acc: 0.9625 | Test Acc: 0.9606\nEpoch 08 | Train Loss: 0.1273 | Train Acc: 0.9662 | Test Acc: 0.9639\nEpoch 09 | Train Loss: 0.1163 | Train Acc: 0.9695 | Test Acc: 0.9672\nEpoch 10 | Train Loss: 0.1068 | Train Acc: 0.9719 | Test Acc: 0.9692\nEpoch 11 | Train Loss: 0.0985 | Train Acc: 0.9746 | Test Acc: 0.9705\nEpoch 12 | Train Loss: 0.0913 | Train Acc: 0.9768 | Test Acc: 0.9722\nEpoch 13 | Train Loss: 0.0848 | Train Acc: 0.9788 | Test Acc: 0.9733\nEpoch 14 | Train Loss: 0.0791 | Train Acc: 0.9803 | Test Acc: 0.9745\nEpoch 15 | Train Loss: 0.0739 | Train Acc: 0.9818 | Test Acc: 0.9761\nEpoch 16 | Train Loss: 0.0692 | Train Acc: 0.9831 | Test Acc: 0.9768\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(jax.devices())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T00:21:40.705842Z","iopub.execute_input":"2025-10-08T00:21:40.706505Z","iopub.status.idle":"2025-10-08T00:21:40.710513Z","shell.execute_reply.started":"2025-10-08T00:21:40.706482Z","shell.execute_reply":"2025-10-08T00:21:40.709665Z"}},"outputs":[{"name":"stdout","text":"[CudaDevice(id=0)]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}