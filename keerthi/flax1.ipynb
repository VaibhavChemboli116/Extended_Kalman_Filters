{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Flax","metadata":{}},{"cell_type":"code","source":"!pip install flax optax jax jaxlib tensorboard tensorflow tensorflow-datasets -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:49:42.179528Z","iopub.execute_input":"2025-10-23T01:49:42.180144Z","iopub.status.idle":"2025-10-23T01:50:01.655399Z","shell.execute_reply.started":"2025-10-23T01:49:42.180114Z","shell.execute_reply":"2025-10-23T01:50:01.654573Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# NN Iib built on top of JAX developed by Google Research (Brain team)\n# Flax was \"designed for flexibility\" hence the name (Flexibility + JAX Flax)\n\nimport flax\nfrom flax .core import freeze, unfreeze\nfrom flax import linen as nn # nn notation also used in PyTorch and in Flax's older API\nfrom flax.training import train_state # a useful dataclass to keep train state\nfrom flax.training import checkpoints\n\n# JAX optimizers-a separate Iib developed by DeepMind\nimport optax\n\n# Flax doesn't have its own data loading functions-we'll be using PyTorch dataloaders\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader\n\n# Python libs\nimport functools # useful utilities for functional programs\nfrom typing import Any, Callable, Sequence, Optional\n\n# Other important 3rd party libs\nimport numpy as np\nimport matplotlib . pyplot as plt\n\nfrom flax.metrics import tensorboard\nfrom functools import partial","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:50:01.657611Z","iopub.execute_input":"2025-10-23T01:50:01.657931Z","iopub.status.idle":"2025-10-23T01:50:29.085052Z","shell.execute_reply.started":"2025-10-23T01:50:01.657903Z","shell.execute_reply":"2025-10-23T01:50:29.083875Z"}},"outputs":[{"name":"stderr","text":"2025-10-23 01:50:08.242428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761184208.466705      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761184208.534605      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport optax\nimport tensorflow_datasets as tfds\nfrom flax import linen as nn\nfrom flax.training import train_state\n\n\n# 1. Load & preprocess MNIST\n\ndef preprocess(image, label):\n    image = jnp.reshape(image, (-1,)) / 255.0   # flatten + normalize\n    return image, jnp.array(label, dtype=jnp.int32)\n\ndef prepare(split, count):\n    ds = tfds.load(\"mnist\", split=split, as_supervised=True)\n    return [preprocess(img, lbl) for img, lbl in tfds.as_numpy(ds.take(count))]\n\ntrain_data = prepare(\"train\", 5000)   # smaller subset for speed\ntest_data  = prepare(\"test\", 1000)\n\n\n# 2. Define MLP with Flax\n\nclass MLP(nn.Module):\n    hidden_dim: int = 128\n    output_dim: int = 10\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(self.hidden_dim)(x)\n        x = nn.relu(x)\n        x = nn.Dense(self.output_dim)(x)\n        return x\n\n\n# 3. Training utilities\n\ndef cross_entropy_loss(logits, labels):\n    one_hot = jax.nn.one_hot(labels, num_classes=10)\n    return -jnp.mean(jnp.sum(one_hot * nn.log_softmax(logits), axis=-1))\n\ndef compute_metrics(logits, labels):\n    loss = cross_entropy_loss(logits, labels)\n    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n    return {\"loss\": loss, \"accuracy\": accuracy}\n\n\n# 4. TrainState (Flax helper)\n\nclass TrainState(train_state.TrainState):\n    pass  # just a container for params + optimizer\n\n\n# 5. Training step\n\n@jax.jit\ndef train_step(state, batch):\n    x, y = batch\n    def loss_fn(params):\n        logits = state.apply_fn(params, x)\n        loss = cross_entropy_loss(logits, y)\n        return loss, logits\n    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    metrics = compute_metrics(logits, y)\n    return state, metrics\n\n@jax.jit\ndef eval_step(state, batch):\n    x, y = batch\n    logits = state.apply_fn(state.params, x)\n    return compute_metrics(logits, y)\n\n# 6. Training loop\n\ndef train_and_evaluate():\n    # Initialize model\n    model = MLP()\n    rng = jax.random.PRNGKey(0)\n    sample_input = jnp.ones((1, 28*28))\n    params = model.init(rng, sample_input)\n\n    # Optimizer\n    tx = optax.adam(1e-3)\n    state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n    # Training loop\n    for epoch in range(5):\n        # Train\n        metrics = []\n        for x, y in train_data:\n            x, y = x[None, :], jnp.array([y])  # add batch dimension\n            state, m = train_step(state, (x, y))\n            metrics.append(m)\n        train_loss = jnp.mean(jnp.array([m[\"loss\"] for m in metrics]))\n        train_acc  = jnp.mean(jnp.array([m[\"accuracy\"] for m in metrics]))\n\n        # Eval\n        metrics = []\n        for x, y in test_data:\n            x, y = x[None, :], jnp.array([y])\n            m = eval_step(state, (x, y))\n            metrics.append(m)\n        test_loss = jnp.mean(jnp.array([m[\"loss\"] for m in metrics]))\n        test_acc  = jnp.mean(jnp.array([m[\"accuracy\"] for m in metrics]))\n\n        print(f\"Epoch {epoch+1}: \"\n              f\"Train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n              f\"Test loss {test_loss:.4f}, acc {test_acc:.4f}\")\n\ntrain_and_evaluate()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:50:29.086036Z","iopub.execute_input":"2025-10-23T01:50:29.086752Z","iopub.status.idle":"2025-10-23T01:51:23.985739Z","shell.execute_reply.started":"2025-10-23T01:50:29.086719Z","shell.execute_reply":"2025-10-23T01:51:23.984976Z"}},"outputs":[{"name":"stdout","text":"Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/mnist/3.0.1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a95be7d818d49febf9360318e08ee46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a643de5a9ac34e8a923042526e8adc5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e32e03380ae41d4b0e13fdd28f86602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"2025-10-23 01:50:33.280976: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/mnist/incomplete.3WXDZT_3.0.1/mnist-train.tfrecord*...:   0%|          | 0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/mnist/incomplete.3WXDZT_3.0.1/mnist-test.tfrecord*...:   0%|          | 0/…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"name":"stderr","text":"INFO:2025-10-23 01:50:56,526:jax._src.xla_bridge:924: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\nINFO:2025-10-23 01:50:56,538:jax._src.xla_bridge:924: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train loss 0.4580, acc 0.8610 | Test loss 0.3680, acc 0.8890\nEpoch 2: Train loss 0.1949, acc 0.9402 | Test loss 0.3120, acc 0.9090\nEpoch 3: Train loss 0.1185, acc 0.9628 | Test loss 0.2876, acc 0.9210\nEpoch 4: Train loss 0.0699, acc 0.9782 | Test loss 0.2228, acc 0.9390\nEpoch 5: Train loss 0.0556, acc 0.9832 | Test loss 0.2015, acc 0.9530\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Toy Transforner\n","metadata":{}},{"cell_type":"markdown","source":"Each image is 28×28.\n\nWe treat it as a sequence of 28 tokens, where each token is a vector of 28 pixels (one row of the image).\n\nThis makes it similar in structure to a text sequence ,perfect for a toy Transformer.","metadata":{}},{"cell_type":"code","source":"def get_data():\n    ds_builder = tfds.builder(\"mnist\")\n    ds_builder.download_and_prepare()\n\n    train_ds = tfds.as_numpy(ds_builder.as_dataset(split=\"train\", batch_size=-1))\n    test_ds  = tfds.as_numpy(ds_builder.as_dataset(split=\"test\", batch_size=-1))\n\n    X_train, y_train = train_ds[\"image\"], train_ds[\"label\"]\n    X_test, y_test   = test_ds[\"image\"], test_ds[\"label\"]\n\n    # Normalize and reshape into (batch, sequence_len=28, feature_dim=28)\n    X_train = X_train.astype(jnp.float32) / 255.0\n    X_test  = X_test.astype(jnp.float32) / 255.0\n    X_train = X_train.reshape(-1, 28, 28)\n    X_test  = X_test.reshape(-1, 28, 28)\n\n    y_train = y_train.astype(jnp.int32)\n    y_test  = y_test.astype(jnp.int32)\n\n    return (X_train, y_train), (X_test, y_test)\n\n(X_train, y_train), (X_test, y_test) = get_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:51:23.988755Z","iopub.execute_input":"2025-10-23T01:51:23.989045Z","iopub.status.idle":"2025-10-23T01:51:27.281579Z","shell.execute_reply.started":"2025-10-23T01:51:23.989023Z","shell.execute_reply":"2025-10-23T01:51:27.280530Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class TinyTransformer(nn.Module):\n    num_heads: int\n    embed_dim: int\n    num_layers: int\n    num_classes: int\n\n    @nn.compact\n    def __call__(self, x):\n        # Linear projection\n        x = nn.Dense(self.embed_dim)(x)\n\n        # Positional encoding (simple sin-cos)\n        positions = jnp.arange(x.shape[1])\n        pos_embed = jnp.sin(positions)[None, :, None]\n        x = x + pos_embed\n\n        # Transformer layers\n        for _ in range(self.num_layers):\n            x = nn.SelfAttention(num_heads=self.num_heads)(x)\n            x = nn.Dense(self.embed_dim)(x)\n            x = nn.LayerNorm()(x)\n\n        # Pool & classify\n        x = jnp.mean(x, axis=1)\n        x = nn.Dense(self.num_classes)(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:51:27.283849Z","iopub.execute_input":"2025-10-23T01:51:27.284138Z","iopub.status.idle":"2025-10-23T01:51:27.292272Z","shell.execute_reply.started":"2025-10-23T01:51:27.284111Z","shell.execute_reply":"2025-10-23T01:51:27.291389Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def create_train_state(rng, model, learning_rate):\n    \"\"\"Initialize model parameters and optimizer.\"\"\"\n    dummy_input = jnp.ones((1, 28, 28))  # match model input shape\n    params = model.init(rng, dummy_input)\n    tx = optax.adam(learning_rate)\n    return train_state.TrainState.create(\n        apply_fn=model.apply, params=params, tx=tx\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:51:27.293385Z","iopub.execute_input":"2025-10-23T01:51:27.294353Z","iopub.status.idle":"2025-10-23T01:51:27.346582Z","shell.execute_reply.started":"2025-10-23T01:51:27.294331Z","shell.execute_reply":"2025-10-23T01:51:27.345823Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"@jax.jit\ndef compute_metrics(logits, labels):\n    \"\"\"Compute loss and accuracy.\"\"\"\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n    return loss, accuracy\n\n\n@jax.jit\ndef train_step(state, batch):\n    \"\"\"Perform a single training step.\"\"\"\n    X, y = batch\n\n    def loss_fn(params):\n        logits = state.apply_fn(params, X)\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n        return loss, logits\n\n    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n    (loss, logits), grads = grad_fn(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    _, accuracy = compute_metrics(logits, y)\n\n    return new_state, loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:51:27.347481Z","iopub.execute_input":"2025-10-23T01:51:27.347727Z","iopub.status.idle":"2025-10-23T01:51:27.362466Z","shell.execute_reply.started":"2025-10-23T01:51:27.347707Z","shell.execute_reply":"2025-10-23T01:51:27.361466Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"rng = jax.random.PRNGKey(0)\nmodel = TinyTransformer(num_heads=2, embed_dim=32, num_layers=2, num_classes=10)\nstate = create_train_state(rng, model, learning_rate=1e-3)\n\nwriter = tensorboard.SummaryWriter(\"./runs/tiny_transformer\")\n\nnum_epochs = 5\nbatch_size = 128\n\nnum_train = len(X_train)\n\nfor epoch in range(num_epochs):\n    rng, input_rng = jax.random.split(rng)\n    perm = jax.random.permutation(input_rng, num_train)\n    X_train_shuf = X_train[perm]\n    y_train_shuf = y_train[perm]\n\n    for i in range(0, num_train, batch_size):\n        X_batch = X_train_shuf[i:i+batch_size]\n        y_batch = y_train_shuf[i:i+batch_size]\n        state, loss, acc = train_step(state, (X_batch, y_batch))\n\n    # Validation\n    logits = model.apply(state.params, X_test)\n    val_loss, val_acc = compute_metrics(logits, y_test)\n\n    print(f\"Epoch {epoch+1}: \"\n          f\"loss={loss:.4f}, acc={acc:.4f}, \"\n          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n\n    writer.scalar(\"train/loss\", float(loss), epoch)\n    writer.scalar(\"train/accuracy\", float(acc), epoch)\n    writer.scalar(\"val/loss\", float(val_loss), epoch)\n    writer.scalar(\"val/accuracy\", float(val_acc), epoch)\n\nwriter.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:51:27.365321Z","iopub.execute_input":"2025-10-23T01:51:27.365614Z","iopub.status.idle":"2025-10-23T01:52:33.701476Z","shell.execute_reply.started":"2025-10-23T01:51:27.365592Z","shell.execute_reply":"2025-10-23T01:52:33.700541Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: loss=0.7003, acc=0.8021, val_loss=0.9551, val_acc=0.6758\nEpoch 2: loss=0.7101, acc=0.7396, val_loss=0.7569, val_acc=0.7517\nEpoch 3: loss=0.8492, acc=0.7083, val_loss=0.6828, val_acc=0.7825\nEpoch 4: loss=0.7256, acc=0.7708, val_loss=0.6435, val_acc=0.7935\nEpoch 5: loss=0.5195, acc=0.8229, val_loss=0.5884, val_acc=0.8079\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir ./runs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:33.702569Z","iopub.execute_input":"2025-10-23T01:52:33.702897Z","iopub.status.idle":"2025-10-23T01:52:41.248502Z","shell.execute_reply.started":"2025-10-23T01:52:33.702877Z","shell.execute_reply":"2025-10-23T01:52:41.247680Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Flax  Selective adaptation","metadata":{}},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.training import train_state, checkpoints\nfrom flax.core import unfreeze, freeze\nimport optax\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:41.249357Z","iopub.execute_input":"2025-10-23T01:52:41.249670Z","iopub.status.idle":"2025-10-23T01:52:41.254472Z","shell.execute_reply.started":"2025-10-23T01:52:41.249649Z","shell.execute_reply":"2025-10-23T01:52:41.253575Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def load_mnist_subset(n_train=5000, n_test=1000, rng_key=0):\n    ds_train = tfds.load(\"mnist\", split=\"train\", batch_size=-1)\n    ds_test  = tfds.load(\"mnist\", split=\"test\",  batch_size=-1)\n    np_train = tfds.as_numpy(ds_train)\n    np_test  = tfds.as_numpy(ds_test)\n\n    X_train = np_train['image'].astype(np.float32) / 255.0  # (60000,28,28,1)\n    y_train = np_train['label'].astype(np.int32)\n    X_test  = np_test['image'].astype(np.float32) / 255.0   # (10000,28,28,1)\n    y_test  = np_test['label'].astype(np.int32)\n\n    # drop last channel and reshape (B,28,28)\n    X_train = X_train.reshape(-1,28,28)[:n_train]\n    y_train = y_train[:n_train]\n    X_test  = X_test.reshape(-1,28,28)[:n_test]\n    y_test  = y_test[:n_test]\n\n    # convert to jnp arrays\n    X_train = jnp.array(X_train)\n    y_train = jnp.array(y_train)\n    X_test  = jnp.array(X_test)\n    y_test  = jnp.array(y_test)\n\n    return (X_train, y_train), (X_test, y_test)\n\n# create shifted dataset by inverting pixel values: x -> 1 - x\n(X_train, y_train), (X_test, y_test) = load_mnist_subset(n_train=5000, n_test=1000)\nX_test_shifted = 1.0 - X_test  # distribution shift: inverted images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:41.255536Z","iopub.execute_input":"2025-10-23T01:52:41.256187Z","iopub.status.idle":"2025-10-23T01:52:44.509600Z","shell.execute_reply.started":"2025-10-23T01:52:41.256158Z","shell.execute_reply":"2025-10-23T01:52:44.508834Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class SmallMLP(nn.Module):\n    hidden_dim: int = 128\n    num_classes: int = 10\n\n    @nn.compact\n    def __call__(self, x):\n        # x shape: (B, 28, 28) -> flatten -> (B, 784)\n        x = x.reshape(x.shape[0], -1)\n        x = nn.Dense(self.hidden_dim)(x)        # Dense_0\n        x = nn.relu(x)\n        x = nn.Dense(self.num_classes)(x)       # Dense_1 (final classifier)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.510884Z","iopub.execute_input":"2025-10-23T01:52:44.511236Z","iopub.status.idle":"2025-10-23T01:52:44.517798Z","shell.execute_reply.started":"2025-10-23T01:52:44.511206Z","shell.execute_reply":"2025-10-23T01:52:44.517091Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def create_train_state(rng, model, learning_rate=1e-3):\n    sample_input = jnp.ones((1, 28, 28), jnp.float32)\n    params = model.init(rng, sample_input)   # returns {'params': { ... }}\n    tx = optax.adam(learning_rate)\n    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.518698Z","iopub.execute_input":"2025-10-23T01:52:44.519255Z","iopub.status.idle":"2025-10-23T01:52:44.535714Z","shell.execute_reply.started":"2025-10-23T01:52:44.519225Z","shell.execute_reply":"2025-10-23T01:52:44.534839Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef cross_entropy_loss(logits, labels):\n    one_hot = jax.nn.one_hot(labels, num_classes=logits.shape[-1])\n    return -jnp.mean(jnp.sum(one_hot * jax.nn.log_softmax(logits), axis=-1))\n\ndef eval_batch(params, apply_fn, X, y):\n    logits = apply_fn(params, X)\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n    acc = jnp.mean(jnp.argmax(logits, -1) == y)\n    return loss, acc\n\n# tell JAX to treat apply_fn as static\neval_batch = jax.jit(eval_batch, static_argnums=(1,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.538518Z","iopub.execute_input":"2025-10-23T01:52:44.538836Z","iopub.status.idle":"2025-10-23T01:52:44.553757Z","shell.execute_reply.started":"2025-10-23T01:52:44.538769Z","shell.execute_reply":"2025-10-23T01:52:44.552907Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def zero_out_grads_except(grads, keep_layer_names):\n    \"\"\"\n    grads: pytree like state.params (FrozenDict with 'params' root)\n    keep_layer_names: list of layer keys to KEEP gradients for, e.g. ['Dense_1']\n    returns: grads with zeros for any parameter not under keep_layer_names\n    \"\"\"\n    g = unfreeze(grads)  # mutable dict\n    # expected structure: {'params': {'Dense_0': {'kernel':..., 'bias':...}, 'Dense_1': {...}}}\n    for layer_name, layer_dict in g['params'].items():\n        if layer_name not in keep_layer_names:\n            # zero out every param array under this layer\n            for param_name in layer_dict:\n                layer_dict[param_name] = jnp.zeros_like(layer_dict[param_name])\n    return freeze(g)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.554629Z","iopub.execute_input":"2025-10-23T01:52:44.554893Z","iopub.status.idle":"2025-10-23T01:52:44.571735Z","shell.execute_reply.started":"2025-10-23T01:52:44.554875Z","shell.execute_reply":"2025-10-23T01:52:44.570955Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport flax\nimport flax.traverse_util\nfrom flax.core import unfreeze, freeze\ndef mask_grads_by_layer(grads, keep_layers):\n    \"\"\"\n    grads: pytree of gradients (same structure as params)\n    keep_layers: tuple/list of layer names to keep (e.g. ('Dense_1',))\n    returns: grads pytree with only specified layers' grads kept\n    \"\"\"\n    mask = make_mask_from_params(grads, keep_layers)\n    return apply_mask_to_grads(grads, mask)\n\ndef make_mask_from_params(params, keep_layer_names):\n    \"\"\"\n    params: frozen params pytree (e.g. state.params)\n    keep_layer_names: iterable of substring names to keep (e.g. ('Dense_1',))\n    returns: a boolean pytree with same structure as params where True means 'update'\n    \"\"\"\n    params_unf = unfreeze(params)\n    p_flat = flax.traverse_util.flatten_dict(params_unf)\n    mask_flat = {}\n    for k in p_flat.keys():\n        key_name = '/'.join(k)\n        keep = any(layer in key_name for layer in keep_layer_names)\n        mask_flat[k] = keep\n    mask_pytree = flax.traverse_util.unflatten_dict(mask_flat)\n    return mask_pytree\n\ndef apply_mask_to_grads(grads, mask_pytree):\n    \"\"\"\n    grads: gradient pytree (same structure as params)\n    mask_pytree: boolean pytree (same structure) produced by make_mask_from_params\n    returns: new grads pytree where masked-out leaves are zeros (same structure)\n    \"\"\"\n    def mask_fn(g, m):\n        return jnp.where(m, g, jnp.zeros_like(g))\n    return jax.tree_map(mask_fn, grads, mask_pytree)\n\ndef train_step_masked(state, batch_x, batch_y, keep_layers):\n    def loss_fn(params):\n        logits = state.apply_fn(state.params, batch_x)\n        loss = cross_entropy_loss(logits, batch_y)\n        return loss, logits\n\n    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n\n    # Apply layer masking\n    grads = mask_grads_by_layer(grads, keep_layers)\n\n    state = state.apply_gradients(grads=grads)\n    acc = jnp.mean(jnp.argmax(logits, -1) == batch_y)\n    return state, loss, acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.572665Z","iopub.execute_input":"2025-10-23T01:52:44.572925Z","iopub.status.idle":"2025-10-23T01:52:44.592185Z","shell.execute_reply.started":"2025-10-23T01:52:44.572905Z","shell.execute_reply":"2025-10-23T01:52:44.591257Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import jax\n\nprint(jax.devices())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.593305Z","iopub.execute_input":"2025-10-23T01:52:44.593621Z","iopub.status.idle":"2025-10-23T01:52:44.611862Z","shell.execute_reply.started":"2025-10-23T01:52:44.593596Z","shell.execute_reply":"2025-10-23T01:52:44.610569Z"}},"outputs":[{"name":"stdout","text":"[CpuDevice(id=0)]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"@jax.jit\ndef train_step_full(state, x, y):\n    def loss_fn(params):\n        logits = state.apply_fn(params, x)\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n        return loss, logits\n    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    acc = jnp.mean(jnp.argmax(logits, -1) == y)\n    return state, loss, acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.612920Z","iopub.execute_input":"2025-10-23T01:52:44.613236Z","iopub.status.idle":"2025-10-23T01:52:44.632032Z","shell.execute_reply.started":"2025-10-23T01:52:44.613207Z","shell.execute_reply":"2025-10-23T01:52:44.631061Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# hyperparams\nrng = jax.random.PRNGKey(0)\nmodel = SmallMLP(hidden_dim=128, num_classes=10)\nstate = create_train_state(rng, model, learning_rate=1e-3)\n\nbatch_size = 128\nnum_epochs = 3  # keep small for demo\n\ndef run_epoch_train(state, X, y, train_step_fn):\n    n = X.shape[0]\n    perm = np.random.permutation(n)\n    Xs = X[perm]\n    ys = y[perm]\n    batch_losses = []\n    batch_accs = []\n    for i in range(0, n, batch_size):\n        xb = Xs[i:i+batch_size]\n        yb = ys[i:i+batch_size]\n        state, loss, acc = train_step_fn(state, xb, yb)\n        batch_losses.append(float(loss))\n        batch_accs.append(float(acc))\n    return state, np.mean(batch_losses), np.mean(batch_accs)\n\nprint(\"Training baseline on original train subset...\")\nfor epoch in range(num_epochs):\n    t0 = time.time()\n    state, loss_tr, acc_tr = run_epoch_train(state, X_train, y_train, train_step_full)\n    loss_val, acc_val = eval_batch(state.params, state.apply_fn, X_test, y_test)\n    print(f\"Epoch {epoch+1}/{num_epochs}  train_loss={loss_tr:.4f} train_acc={acc_tr:.4f}  test_acc={float(acc_val):.4f}  time={time.time()-t0:.1f}s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:44.632944Z","iopub.execute_input":"2025-10-23T01:52:44.633318Z","iopub.status.idle":"2025-10-23T01:52:46.244070Z","shell.execute_reply.started":"2025-10-23T01:52:44.633293Z","shell.execute_reply":"2025-10-23T01:52:46.243281Z"}},"outputs":[{"name":"stdout","text":"Training baseline on original train subset...\nEpoch 1/3  train_loss=1.2050 train_acc=0.7127  test_acc=0.8850  time=1.4s\nEpoch 2/3  train_loss=0.4708 train_acc=0.8740  test_acc=0.9020  time=0.1s\nEpoch 3/3  train_loss=0.3407 train_acc=0.9068  test_acc=0.9180  time=0.1s\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"loss_orig, acc_orig = eval_batch(state.params, state.apply_fn, X_test, y_test)\nloss_shift, acc_shift = eval_batch(state.params, state.apply_fn, X_test_shifted, y_test)\nprint(f\"Baseline test accuracy (orig): {float(acc_orig):.4f}\")\nprint(f\"Baseline test accuracy (shifted): {float(acc_shift):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:46.244947Z","iopub.execute_input":"2025-10-23T01:52:46.245253Z","iopub.status.idle":"2025-10-23T01:52:46.254951Z","shell.execute_reply.started":"2025-10-23T01:52:46.245214Z","shell.execute_reply":"2025-10-23T01:52:46.254227Z"}},"outputs":[{"name":"stdout","text":"Baseline test accuracy (orig): 0.9180\nBaseline test accuracy (shifted): 0.0070\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import copy\nstate_selective = copy.deepcopy(state)\n# hyperparams\nrng = jax.random.PRNGKey(0)\nmodel = SmallMLP(hidden_dim=128, num_classes=10)\nstate = create_train_state(rng, model, learning_rate=1e-3)\n\n# Copy state for selective retrain\nstate_selective = copy.deepcopy(state)\n\n\n# We'll train only Dense_1 (final classifier). Name in Linen is 'Dense_1'\nkeep_layers = ('Dense_1',)  # Changed to a tuple\n\nnum_adapt_epochs = 3\nprint(\"\\nSelective adaptation (only final Dense_1) on shifted data...\")\nfor epoch in range(num_adapt_epochs):\n    state_selective, loss_tr, acc_tr = run_epoch_train(state_selective, X_test_shifted, y_test,\n                                                      lambda st, xb, yb: train_step_masked(st, xb, yb, keep_layers))\n    loss_val, acc_val = eval_batch(state_selective.params, state_selective.apply_fn, X_test_shifted, y_test)\n    print(f\"Adapt Epoch {epoch+1}/{num_adapt_epochs}  train_loss={loss_tr:.4f} train_acc={acc_tr:.4f}  shifted_acc={float(acc_val):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:46.256025Z","iopub.execute_input":"2025-10-23T01:52:46.256670Z","iopub.status.idle":"2025-10-23T01:52:49.397240Z","shell.execute_reply.started":"2025-10-23T01:52:46.256650Z","shell.execute_reply":"2025-10-23T01:52:49.396277Z"}},"outputs":[{"name":"stdout","text":"\nSelective adaptation (only final Dense_1) on shifted data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/1045712083.py:39: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n  return jax.tree_map(mask_fn, grads, mask_pytree)\n","output_type":"stream"},{"name":"stdout","text":"Adapt Epoch 1/3  train_loss=2.4096 train_acc=0.1189  shifted_acc=0.1190\nAdapt Epoch 2/3  train_loss=2.4126 train_acc=0.1191  shifted_acc=0.1190\nAdapt Epoch 3/3  train_loss=2.4111 train_acc=0.1178  shifted_acc=0.1190\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Reset to baseline and do full retrain (update all params) for same number of epochs\nstate_full_retrain = copy.deepcopy(state)\nprint(\"\\nFull retrain (all params) on shifted data...\")\nfor epoch in range(num_adapt_epochs):\n    state_full_retrain, loss_tr, acc_tr = run_epoch_train(state_full_retrain, X_test_shifted, y_test, train_step_full)\n    loss_val, acc_val = eval_batch(state_full_retrain.params, state_full_retrain.apply_fn, X_test_shifted, y_test)\n    print(f\"FullAdapt Epoch {epoch+1}/{num_adapt_epochs}  train_loss={loss_tr:.4f} train_acc={acc_tr:.4f}  shifted_acc={float(acc_val):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:49.398179Z","iopub.execute_input":"2025-10-23T01:52:49.398512Z","iopub.status.idle":"2025-10-23T01:52:50.482006Z","shell.execute_reply.started":"2025-10-23T01:52:49.398491Z","shell.execute_reply":"2025-10-23T01:52:50.481049Z"}},"outputs":[{"name":"stdout","text":"\nFull retrain (all params) on shifted data...\nFullAdapt Epoch 1/3  train_loss=2.2669 train_acc=0.1769  shifted_acc=0.3420\nFullAdapt Epoch 2/3  train_loss=1.9635 train_acc=0.4060  shifted_acc=0.5460\nFullAdapt Epoch 3/3  train_loss=1.7187 train_acc=0.5647  shifted_acc=0.6250\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"_, acc_selective = eval_batch(state_selective.params, state_selective.apply_fn, X_test_shifted, y_test)\n_, acc_full = eval_batch(state_full_retrain.params, state_full_retrain.apply_fn, X_test_shifted, y_test)\n_, acc_orig_after_selective = eval_batch(state_selective.params, state_selective.apply_fn, X_test, y_test)\n_, acc_orig_after_full = eval_batch(state_full_retrain.params, state_full_retrain.apply_fn, X_test, y_test)\n\nprint(\"\\nFinal comparison on SHIFTED data:\")\nprint(f\"Selective-adapt acc (only Dense_1 updated): {float(acc_selective):.4f}\")\nprint(f\"Full-retrain acc (all params updated): {float(acc_full):.4f}\")\n\nprint(\"\\nEffect on ORIGINAL data after adaptation:\")\nprint(f\"Selective-adapt acc on original: {float(acc_orig_after_selective):.4f}\")\nprint(f\"Full-retrain acc on original:    {float(acc_orig_after_full):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T01:52:50.483048Z","iopub.execute_input":"2025-10-23T01:52:50.483324Z","iopub.status.idle":"2025-10-23T01:52:50.499253Z","shell.execute_reply.started":"2025-10-23T01:52:50.483296Z","shell.execute_reply":"2025-10-23T01:52:50.498309Z"}},"outputs":[{"name":"stdout","text":"\nFinal comparison on SHIFTED data:\nSelective-adapt acc (only Dense_1 updated): 0.1190\nFull-retrain acc (all params updated): 0.6250\n\nEffect on ORIGINAL data after adaptation:\nSelective-adapt acc on original: 0.1150\nFull-retrain acc on original:    0.0010\n","output_type":"stream"}],"execution_count":23}]}