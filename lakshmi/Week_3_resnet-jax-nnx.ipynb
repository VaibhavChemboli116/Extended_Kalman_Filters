{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- Imports ----------\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nimport tensorflow_datasets as tfds\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom typing import List\n\n# ---------- Define Residual Block ----------\nclass ResidualBlock(nnx.Module):\n    def __init__(self, in_channels: int, out_channels: int, stride: int = 1, rngs: nnx.Rngs = None):\n        self.conv1 = nnx.Conv(\n            in_features=in_channels, \n            out_features=out_channels,\n            kernel_size=(3, 3), \n            strides=(stride, stride),\n            padding=\"SAME\", \n            rngs=rngs\n        )\n        self.bn1 = nnx.BatchNorm(num_features=out_channels, rngs=rngs)\n        \n        self.conv2 = nnx.Conv(\n            in_features=out_channels, \n            out_features=out_channels,\n            kernel_size=(3, 3), \n            strides=(1, 1),\n            padding=\"SAME\", \n            rngs=rngs\n        )\n        self.bn2 = nnx.BatchNorm(num_features=out_channels, rngs=rngs)\n        \n        # Shortcut connection\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nnx.Conv(\n                in_features=in_channels, \n                out_features=out_channels,\n                kernel_size=(1, 1), \n                strides=(stride, stride),\n                padding=\"SAME\", \n                rngs=rngs\n            )\n            self.has_projection = True\n        else:\n            self.has_projection = False\n    \n    def __call__(self, x):\n        identity = self.shortcut(x) if self.has_projection else x\n        \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = nnx.relu(out)\n        \n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        out = out + identity\n        out = nnx.relu(out)\n        \n        return out\n\n# ---------- Define Small ResNet with Indexable Layers ----------\nclass ResNetSmall(nnx.Module):\n    def __init__(self, num_classes: int = 10, rngs: nnx.Rngs = None):\n        # Layer 0: Stem\n        self.stem = nnx.Conv(\n            in_features=1, \n            out_features=16, \n            kernel_size=(3, 3),\n            padding=\"SAME\", \n            rngs=rngs\n        )\n        self.stem_bn = nnx.BatchNorm(num_features=16, rngs=rngs)\n        \n        # Layers 1-3: Residual blocks\n        self.block1 = ResidualBlock(16, 16, stride=1, rngs=rngs)\n        self.block2 = ResidualBlock(16, 32, stride=2, rngs=rngs)\n        self.block3 = ResidualBlock(32, 64, stride=2, rngs=rngs)\n        \n        # Layer 4: Classifier\n        self.fc = nnx.Linear(in_features=64, out_features=num_classes, rngs=rngs)\n        \n        # Store layer names for probing\n        self.layer_names = ['stem', 'block1', 'block2', 'block3', 'fc']\n    \n    def __call__(self, x, return_intermediates: bool = False):\n        intermediates = {}\n        \n        # Layer 0: Stem\n        x = self.stem(x)\n        x = self.stem_bn(x)\n        x = nnx.relu(x)\n        if return_intermediates:\n            intermediates['stem'] = x\n        \n        # Layer 1: Block 1\n        x = self.block1(x)\n        if return_intermediates:\n            intermediates['block1'] = x\n        \n        # Layer 2: Block 2\n        x = self.block2(x)\n        if return_intermediates:\n            intermediates['block2'] = x\n        \n        # Layer 3: Block 3\n        x = self.block3(x)\n        if return_intermediates:\n            intermediates['block3'] = x\n        \n        # Global average pooling\n        x = jnp.mean(x, axis=(1, 2))\n        \n        # Layer 4: Classifier\n        x = self.fc(x)\n        if return_intermediates:\n            intermediates['fc'] = x\n        \n        if return_intermediates:\n            return x, intermediates\n        return x\n    \n    def get_layer(self, layer_idx: int):\n        \"\"\"Get layer by index for probing\"\"\"\n        layers = [self.stem, self.block1, self.block2, self.block3, self.fc]\n        if 0 <= layer_idx < len(layers):\n            return layers[layer_idx]\n        raise IndexError(f\"Layer index {layer_idx} out of range [0, {len(layers)-1}]\")\n\n# ---------- Load MNIST ----------\ndef load_mnist(batch_size: int = 128):\n    ds_builder = tfds.builder(\"mnist\")\n    ds_builder.download_and_prepare()\n    train_ds = ds_builder.as_dataset(split=\"train\", as_supervised=True)\n    test_ds = ds_builder.as_dataset(split=\"test\", as_supervised=True)\n    \n    def preprocess(image, label):\n        image = jnp.array(image, dtype=jnp.float32) / 255.0\n        label = jnp.array(label, dtype=jnp.int32)\n        return image, label\n    \n    train_data = [(preprocess(img, lbl)) for img, lbl in tfds.as_numpy(train_ds)]\n    test_data = [(preprocess(img, lbl)) for img, lbl in tfds.as_numpy(test_ds)]\n    \n    return train_data, test_data\n\n# ---------- Batching Function ----------\ndef create_batches(data, batch_size: int):\n    \"\"\"Create batches from data\"\"\"\n    batches = []\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i + batch_size]\n        images = jnp.stack([img for img, _ in batch])\n        labels = jnp.array([lbl for _, lbl in batch])\n        batches.append((images, labels))\n    return batches\n\n# ---------- Training Functions ----------\n@nnx.jit\ndef train_step(model, optimizer, x, y):\n    \"\"\"Single training step\"\"\"\n    def loss_fn(model):\n        logits = model(x)\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n        return loss, logits\n    \n    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n    (loss, logits), grads = grad_fn(model)\n    optimizer.update(grads)\n    \n    acc = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return loss, acc\n\n@nnx.jit\ndef eval_step(model, x, y):\n    \"\"\"Single evaluation step\"\"\"\n    logits = model(x)\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n    acc = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return loss, acc\n\n# ---------- Probing Function ----------\ndef probe_layers(model, x, layer_indices: List[int] = None):\n    \"\"\"\n    Probe intermediate layer activations\n    \n    Args:\n        model: The ResNet model\n        x: Input batch\n        layer_indices: List of layer indices to probe (None = all layers)\n    \n    Returns:\n        Dictionary mapping layer names to their activations\n    \"\"\"\n    logits, intermediates = model(x, return_intermediates=True)\n    \n    if layer_indices is not None:\n        filtered = {}\n        for idx in layer_indices:\n            if 0 <= idx < len(model.layer_names):\n                name = model.layer_names[idx]\n                filtered[name] = intermediates[name]\n        return filtered\n    \n    return intermediates\n\n# ---------- Main Training Loop ----------\ndef main():\n    # Initialize model\n    print(\"Initializing model...\")\n    rngs = nnx.Rngs(0)\n    model = ResNetSmall(num_classes=10, rngs=rngs)\n    \n    # Initialize optimizer\n    optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n    \n    # Load data\n    print(\"Loading MNIST dataset...\")\n    train_data, test_data = load_mnist()\n    print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}\")\n    \n    # Visualize samples\n    print(\"\\nVisualizing sample images...\")\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    for i in range(3):\n        img, label = train_data[i]\n        axes[i].imshow(img.squeeze(), cmap=\"gray\")\n        axes[i].set_title(f\"Label: {int(label)}\")\n        axes[i].axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n    \n    # Create batches\n    batch_size = 128\n    train_batches = create_batches(train_data, batch_size)\n    test_batches = create_batches(test_data, batch_size)\n    \n    # Training loop\n    num_epochs = 5\n    print(f\"\\nTraining for {num_epochs} epochs...\")\n    \n    for epoch in range(num_epochs):\n        # Training\n        train_loss_sum = 0.0\n        train_acc_sum = 0.0\n        \n        for x_batch, y_batch in tqdm(train_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            loss, acc = train_step(model, optimizer, x_batch, y_batch)\n            train_loss_sum += loss\n            train_acc_sum += acc\n        \n        train_loss = train_loss_sum / len(train_batches)\n        train_acc = train_acc_sum / len(train_batches)\n        \n        # Evaluation\n        test_loss_sum = 0.0\n        test_acc_sum = 0.0\n        \n        for x_batch, y_batch in test_batches:\n            loss, acc = eval_step(model, x_batch, y_batch)\n            test_loss_sum += loss\n            test_acc_sum += acc\n        \n        test_loss = test_loss_sum / len(test_batches)\n        test_acc = test_acc_sum / len(test_batches)\n        \n        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n              f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n    \n    # ---------- Layer Probing Demo ----------\n    print(\"\\n\" + \"=\"*50)\n    print(\"LAYER PROBING DEMONSTRATION\")\n    print(\"=\"*50)\n    \n    # Get a sample batch\n    sample_x, sample_y = test_batches[0]\n    sample_x_single = sample_x[:1]  # Single image\n    \n    # Probe all layers\n    print(\"\\nProbing all layers...\")\n    all_activations = probe_layers(model, sample_x_single)\n    \n    for layer_name, activation in all_activations.items():\n        print(f\"{layer_name}: shape = {activation.shape}\")\n    \n    # Probe specific layers\n    print(\"\\nProbing specific layers [1, 3]...\")\n    specific_activations = probe_layers(model, sample_x_single, layer_indices=[1, 3])\n    \n    for layer_name, activation in specific_activations.items():\n        print(f\"{layer_name}: shape = {activation.shape}\")\n    \n    # Access layers by index\n    print(\"\\nAccessing layers by index...\")\n    for i in range(len(model.layer_names)):\n        layer = model.get_layer(i)\n        print(f\"Layer {i} ({model.layer_names[i]}): {type(layer).__name__}\")\n    \n    return model, optimizer\n\nif __name__ == \"__main__\":\n    model, optimizer = main()\n    print(\"\\nTraining complete! Model is ready for EKF fine-tuning on CIFAR.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T15:41:25.320858Z","iopub.execute_input":"2025-10-15T15:41:25.321168Z","iopub.status.idle":"2025-10-15T15:43:47.385590Z","shell.execute_reply.started":"2025-10-15T15:41:25.321151Z","shell.execute_reply":"2025-10-15T15:43:47.384901Z"}},"outputs":[{"name":"stdout","text":"Initializing model...\nLoading MNIST dataset...\nTrain samples: 60000, Test samples: 10000\n\nVisualizing sample images...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 3 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nTraining for 5 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 469/469 [00:12<00:00, 37.64it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.3566, Train Acc=0.9118, Test Loss=0.0716, Test Acc=0.9802\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 469/469 [00:02<00:00, 211.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.0614, Train Acc=0.9835, Test Loss=0.0465, Test Acc=0.9856\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 469/469 [00:02<00:00, 216.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.0415, Train Acc=0.9885, Test Loss=0.0405, Test Acc=0.9866\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 469/469 [00:02<00:00, 170.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.0314, Train Acc=0.9916, Test Loss=0.0365, Test Acc=0.9880\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 469/469 [00:02<00:00, 209.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.0249, Train Acc=0.9936, Test Loss=0.0322, Test Acc=0.9896\n\n==================================================\nLAYER PROBING DEMONSTRATION\n==================================================\n\nProbing all layers...\nstem: shape = (1, 28, 28, 16)\nblock1: shape = (1, 28, 28, 16)\nblock2: shape = (1, 14, 14, 32)\nblock3: shape = (1, 7, 7, 64)\nfc: shape = (1, 10)\n\nProbing specific layers [1, 3]...\nblock1: shape = (1, 28, 28, 16)\nblock3: shape = (1, 7, 7, 64)\n\nAccessing layers by index...\nLayer 0 (stem): Conv\nLayer 1 (block1): ResidualBlock\nLayer 2 (block2): ResidualBlock\nLayer 3 (block3): ResidualBlock\nLayer 4 (fc): Linear\n\nTraining complete! Model is ready for EKF fine-tuning on CIFAR.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}